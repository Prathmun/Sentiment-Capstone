{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "honey-cardiff",
   "metadata": {},
   "source": [
    "# Imports and basic set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "original-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the begining, ther is time\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "quiet-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quality of life\n",
    "#Some things we do generate a lot of warnings, and it just becomes clutter.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#I like it when my notebook helps me out\n",
    "%config IPCompleter.greedy=True\n",
    "#Sometiems you just need to print pretty\n",
    "from pprint import pprint\n",
    "#Lots of these operations take many minutes to complete. So it behooves us to pickle the outputs and just unpickle them each time we re-open the notebook\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-patio",
   "metadata": {},
   "source": [
    "Data is from: https://www.kaggle.com/datasets/toygarr/datasets-for-natural-language-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-yahoo",
   "metadata": {},
   "source": [
    "Dataset is from a collection of sentiment datasets, but we just want to play with the food one for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-brake",
   "metadata": {},
   "source": [
    "\"ctweet, stweet, food\" datasets are positive or negative analysis (sentiment) -> 0 negative -> 1 positive (ctweet has neutral 0, 1, 2)\n",
    "\n",
    "we're assuming that any code we write to deal with this food data set will be extensible later by simply adding the rest of the data if we so choose. At the moment it's simply faster to work with a smaller subset of the data as we design the pipeline and it's displays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-length",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "north-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import ngrams\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-fancy",
   "metadata": {},
   "source": [
    "## Corpus and Vectoring tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "final-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "pleased-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-worse",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "thirty-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('C:/Users/Prathmun/Documents/Springboard Jupyter/Sentiment-Capstone/data/food/test.csv')\n",
    "train = pd.read_csv('C:/Users/Prathmun/Documents/Springboard Jupyter/Sentiment-Capstone/data/food/train.csv')\n",
    "frames = [train, test]\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-pontiac",
   "metadata": {},
   "source": [
    "Nobs to play with, want 'em nearby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "medical-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 50000\n",
    "num_feats =10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-carnival",
   "metadata": {},
   "source": [
    "# Sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-intensity",
   "metadata": {},
   "source": [
    "Here we can set up the code to downsample for speed's sake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "elementary-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "conditional-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample_size = 50000\n",
    "# df = resample(df,\n",
    "#                          replace=False,\n",
    "#                          n_samples=sample_size,\n",
    "#                         random_state=12,\n",
    "#                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "alternate-christian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363219, 2)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "considerable-franklin",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    306223\n",
       "0     56996\n",
       "Name: Y, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-little",
   "metadata": {},
   "source": [
    "# Processing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-limitation",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "noted-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df.text.apply(word_tokenize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "crucial-creation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Y</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i was put off at first by the green powder but...</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, was, put, off, at, first, by, the, green, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these ginger chews are too good to be true i t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[these, ginger, chews, are, too, good, to, be,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Y  \\\n",
       "0  i was put off at first by the green powder but...  1   \n",
       "1  these ginger chews are too good to be true i t...  1   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [i, was, put, off, at, first, by, the, green, ...  \n",
       "1  [these, ginger, chews, are, too, good, to, be,...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-cancellation",
   "metadata": {},
   "source": [
    "### Remove Stop Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "abroad-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "intensive-berkeley",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['nostops'] = df.tokenized.apply(lambda  x: [item for item in x if item not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "healthy-introduction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Y</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>nostops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i was put off at first by the green powder but...</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, was, put, off, at, first, by, the, green, ...</td>\n",
       "      <td>[put, first, green, powder, bad, little, grain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these ginger chews are too good to be true i t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[these, ginger, chews, are, too, good, to, be,...</td>\n",
       "      <td>[ginger, chews, good, true, try, limit, one, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Y  \\\n",
       "0  i was put off at first by the green powder but...  1   \n",
       "1  these ginger chews are too good to be true i t...  1   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [i, was, put, off, at, first, by, the, green, ...   \n",
       "1  [these, ginger, chews, are, too, good, to, be,...   \n",
       "\n",
       "                                             nostops  \n",
       "0  [put, first, green, powder, bad, little, grain...  \n",
       "1  [ginger, chews, good, true, try, limit, one, d...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-prospect",
   "metadata": {},
   "source": [
    "### Stemming \n",
    "Might do lemmatization later, but stemming is simpler and more reliable and we're looking to get something working before we refine it. Lemmatization is a step we can experiment with in our refinement stages\n",
    "\n",
    "We chose snowball because it's an older, and stable stemmer that incorporates improvements from the older stemmer algorithm Porter's real world experience\n",
    "Snowball docs: https://www.nltk.org/api/nltk.stem.snowball.html\n",
    "Article that informed our decision to choose Snowball: https://towardsdatascience.com/stemming-lemmatization-what-ba782b7c0bd8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-victory",
   "metadata": {},
   "source": [
    "It looks as if stemming can increase recall, even in short texts but can also cause problems. We're going to move forward with stemmed words for now, but again can return to this if we seek optimization tasks down the line\n",
    "Source: https://stackoverflow.com/questions/47219389/compute-word-n-grams-on-original-text-or-after-lemma-stemming-process#:~:text=Computing%20word%20n%2Dgrams%20after,you%20want%20to%20do%20it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "terminal-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "df['stemmed'] = df.nostops.apply(lambda x: [stemmer.stem(item) for item in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "physical-milton",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [put, first, green, powder, bad, littl, graini...\n",
       "1    [ginger, chew, good, true, tri, limit, one, da...\n",
       "Name: stemmed, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stemmed.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-bhutan",
   "metadata": {},
   "source": [
    "# Building Limited Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "alert-monitor",
   "metadata": {},
   "outputs": [],
   "source": [
    "nostops_text_neg = \" \".join(\" \".join(listo) for listo in df.nostops[df['Y'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "stuck-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_list = nostops_text_neg.split(' ')\n",
    "neg_set = set(neg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "surprising-farmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "nostops_text_pos = \" \".join(\" \".join(listo) for listo in df.nostops[df['Y'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "micro-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = nostops_text_pos.split(' ')\n",
    "pos_set = set(pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "productive-wilderness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg list len = 2334177\n",
      "neg set len = 46886\n"
     ]
    }
   ],
   "source": [
    "print('neg list len = ' + str(len(neg_list)))\n",
    "print('neg set len = ' + str(len(neg_set))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "treated-coordinate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos list len = 11470406\n",
      "pos set len = 104877\n"
     ]
    }
   ],
   "source": [
    "print('pos list len = ' + str(len(pos_list)))\n",
    "print('pos set len = ' + str(len(pos_set))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-dinner",
   "metadata": {},
   "source": [
    "## Ratio DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-collins",
   "metadata": {},
   "source": [
    "## Extracting Negative Only words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "rocky-editor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11892"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_only_set = neg_set - pos_set\n",
    "len(neg_only_set )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "engaged-estimate",
   "metadata": {},
   "source": [
    "### Neg only Count DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "aging-appraisal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11892"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building up the count for the neg only df so that we can order them to slice, so we can add them to the lexicon we're going to feed into the vocabulary of the tokenizer\n",
    "\n",
    "neg_count = {}\n",
    "\n",
    "for word in neg_list:\n",
    "    if word in neg_only_set:\n",
    "        if word not in neg_count.keys():\n",
    "            \n",
    "            neg_count[word] = 0\n",
    "            \n",
    "        neg_count[word] += 1\n",
    "        \n",
    "neg_count_df = pd.DataFrame(neg_count, index=[0])\n",
    "                            \n",
    "neg_only_count_df = neg_count_df.transpose()\n",
    "\n",
    "len(neg_only_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-cutting",
   "metadata": {},
   "source": [
    "## Extracting Positive Only Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "trying-fisher",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69883"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_only_set =  pos_set - neg_set \n",
    "len(pos_only_set )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "german-friendly",
   "metadata": {},
   "source": [
    "### Pos Only Count Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "hybrid-triple",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69883"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building up the count for the neg only df so that we can order them to slice, so we can add them to the lexicon we're going to feed into the vocabulary of the tokenizer\n",
    "\n",
    "pos_count = {}\n",
    "\n",
    "for word in pos_list:\n",
    "    if word in pos_only_set:\n",
    "        if word not in pos_count.keys():\n",
    "            \n",
    "            pos_count[word] = 0\n",
    "            \n",
    "        pos_count[word] += 1\n",
    "        \n",
    "pos_count_df = pd.DataFrame(pos_count, index=[0])\n",
    "                            \n",
    "pos_only_count_df = pos_count_df.transpose()\n",
    "\n",
    "len(pos_only_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-tractor",
   "metadata": {},
   "source": [
    "### Extracting word ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-wheat",
   "metadata": {},
   "source": [
    "Using set logic to snag words that only appear in both positive and negative polarity documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "adjacent-explanation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34994"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_set = neg_set.intersection(pos_set)\n",
    "len(shared_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "collaborative-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_df =  pd.DataFrame(shared_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dietary-documentation",
   "metadata": {},
   "source": [
    "### Counting the negative appearences of our set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "comprehensive-florence",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_count = {}\n",
    "\n",
    "for word in neg_list:\n",
    "    if word in shared_set:\n",
    "        if word not in neg_count.keys():\n",
    "            \n",
    "            neg_count[word] = 0\n",
    "            \n",
    "        neg_count[word] += 1\n",
    "        \n",
    "neg_count_df = pd.DataFrame(neg_count, index=[0])\n",
    "                            \n",
    "neg_count_df = neg_count_df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "amazing-citizen",
   "metadata": {},
   "source": [
    "### Counting the positive appearences of our set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "genuine-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_count = {}\n",
    "\n",
    "for word in pos_list:\n",
    "    if word in shared_set:\n",
    "        if word not in pos_count.keys():\n",
    "            \n",
    "            pos_count[word] = 0\n",
    "            \n",
    "        pos_count[word] += 1\n",
    "\n",
    "pos_count_df = pd.DataFrame(pos_count, index=[0])\n",
    "\n",
    "pos_count_df = pos_count_df.transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "floating-shaft",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stapling the pos/neg counts to the ratio df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "forbidden-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_df['neg_count'] = neg_count_df[0]\n",
    "ratio_df['pos_count'] = pos_count_df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-kentucky",
   "metadata": {},
   "source": [
    "### Getting ratiod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "digital-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_df['ratio'] = ratio_df[['neg_count','pos_count']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-placement",
   "metadata": {},
   "source": [
    "# Function to slice the lexicons appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "mineral-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicer(pos_only, neg_only, high_ratio):\n",
    "    \n",
    "    #slice pos\n",
    "    pos_slice = list(pos_only_count_df.sort_values(by=[0], ascending=False).head(pos_only).index)\n",
    "    print(len(pos_slice))\n",
    "    #slice neg\n",
    "    neg_slice = list(neg_only_count_df.sort_values(by=[0], ascending=False).head(neg_only).index)\n",
    "    print(len(neg_slice))\n",
    "    #slice high_ratio\n",
    "    ratio_slice = list(ratio_df.sort_values(ascending=False, by='ratio').head(high_ratio+1).index)[1:]\n",
    "    print(len(ratio_slice))\n",
    "    \n",
    "    \n",
    "    #Join slices into one big list\n",
    "    sliced_lexicons = [pos_slice,neg_slice,ratio_slice]\n",
    "    sliced_lexicon = []\n",
    "    for lexicon in sliced_lexicons:\n",
    "        for word in lexicon:\n",
    "            sliced_lexicon.append(word)\n",
    "        \n",
    "    #Return list\n",
    "    return sliced_lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-timber",
   "metadata": {},
   "source": [
    "# <span style=\"color:purple\"> Clustering and Secondary EDA/Feature Engineering</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-river",
   "metadata": {},
   "source": [
    "We played briefly with using DBSCAN, but it didn't converge at all, it gave us 360k categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-victorian",
   "metadata": {},
   "source": [
    "Thinking that instead of conventional clustering we're going to play with latent dirichlet allocation because it'll do a similar thing as K-means, but also provide us with a lot more information about the composition of those clusters.\n",
    "Original Paper: https://ai.stanford.edu/~ang/papers/nips01-lda.pdf\n",
    "Tutorial we used to help us get going: https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "Docs for the gensim module we're sdoing the lda with: https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "european-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_texts = list(df.stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "nuclear-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes a dictionary mapping to unique IDs\n",
    "id2word = Dictionary(stemmed_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "insured-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes a bag of words version of the Gensim Dictionary\n",
    "corpus = [id2word.doc2bow(text) for text in stemmed_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "persistent-butter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this took 75.45151829719543 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# # Train the model on the corpus.\n",
    "start = time.time()\n",
    "num_topics = 150\n",
    "lda = LdaModel(corpus,  id2word=id2word, num_topics=num_topics)\n",
    "stop = time.time()\n",
    "print(\"training the model took \" +str(stop-start) + \" seconds to complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-termination",
   "metadata": {},
   "source": [
    "# Feature Engineering with LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-thought",
   "metadata": {},
   "source": [
    "## Topic Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-hungarian",
   "metadata": {},
   "source": [
    "Documentation we're going to need to use to get at document relationships to topics: https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-taste",
   "metadata": {},
   "source": [
    "Relevant stack overflow\n",
    "https://stackoverflow.com/questions/43357247/get-document-topics-and-get-term-topics-in-gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "broke-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "liquid-commercial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this took 58.73469805717468 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#Stole the below code from\n",
    "#https://stackoverflow.com/questions/46574720/python-gensim-lda-add-the-topic-to-the-document-after-getting-the-topics\n",
    "all_topics = lda.get_document_topics(corpus, minimum_probability=0.0)\n",
    "all_topics_csr = gensim.matutils.corpus2csc(all_topics)\n",
    "all_topics_numpy = all_topics_csr.T.toarray()\n",
    "all_topics_df = pd.DataFrame(all_topics_numpy)\n",
    "stop = time.time()\n",
    "print(\"pulling the topcs took \" +str(stop-start) + \" seconds to complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "gorgeous-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a feature of designing our code with downsampled data. Just need the index's to be consistent.\n",
    "all_topics_df.index = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "vanilla-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's rename the topic columns so that we can identify them later on\n",
    "for each in all_topics_df.columns:\n",
    "    all_topics_df['topic_' + str(each)] = all_topics_df[each]\n",
    "    all_topics_df.drop(columns=[each], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "working-gallery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_110</th>\n",
       "      <th>topic_111</th>\n",
       "      <th>topic_112</th>\n",
       "      <th>topic_113</th>\n",
       "      <th>topic_114</th>\n",
       "      <th>topic_115</th>\n",
       "      <th>topic_116</th>\n",
       "      <th>topic_117</th>\n",
       "      <th>topic_118</th>\n",
       "      <th>topic_119</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33425</th>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "      <td>0.000269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174759</th>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111229</th>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.000333</td>\n",
       "      <td>0.048490</td>\n",
       "      <td>0.000333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209101</th>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.313755</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "      <td>0.000397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109643</th>\n",
       "      <td>0.145762</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.106666</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.000758</td>\n",
       "      <td>0.135033</td>\n",
       "      <td>0.000758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59447</th>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.069769</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.169432</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.029456</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.024203</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>0.018131</td>\n",
       "      <td>0.012412</td>\n",
       "      <td>0.000096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97003</th>\n",
       "      <td>0.089298</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.223862</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.053188</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000362</td>\n",
       "      <td>0.000362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64465</th>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.069206</td>\n",
       "      <td>0.026357</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.023418</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.065573</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.029271</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.057708</td>\n",
       "      <td>0.000181</td>\n",
       "      <td>0.000181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91943</th>\n",
       "      <td>0.011404</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.015598</td>\n",
       "      <td>0.041362</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.007600</td>\n",
       "      <td>0.016370</td>\n",
       "      <td>0.004398</td>\n",
       "      <td>0.006995</td>\n",
       "      <td>...</td>\n",
       "      <td>0.088138</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.016547</td>\n",
       "      <td>0.012009</td>\n",
       "      <td>0.012541</td>\n",
       "      <td>0.013296</td>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.030237</td>\n",
       "      <td>0.008304</td>\n",
       "      <td>0.000023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141385</th>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.059193</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.201535</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "      <td>0.000309</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows Ã— 120 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_0   topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "33425   0.000269  0.000269  0.000269  0.000269  0.000269  0.000269  0.000269   \n",
       "174759  0.000309  0.000309  0.000309  0.000309  0.000309  0.000309  0.000309   \n",
       "111229  0.000333  0.000333  0.000333  0.000333  0.000333  0.000333  0.000333   \n",
       "209101  0.000397  0.313755  0.000397  0.000397  0.000397  0.000397  0.000397   \n",
       "109643  0.145762  0.000758  0.000758  0.000758  0.000758  0.000758  0.000758   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "59447   0.000096  0.000096  0.000096  0.000096  0.069769  0.000096  0.169432   \n",
       "97003   0.089298  0.000362  0.000362  0.000362  0.000362  0.000362  0.223862   \n",
       "64465   0.000181  0.000181  0.069206  0.026357  0.000181  0.023418  0.000181   \n",
       "91943   0.011404  0.000023  0.000023  0.015598  0.041362  0.000023  0.007600   \n",
       "141385  0.000309  0.000309  0.000309  0.000309  0.000309  0.000309  0.000309   \n",
       "\n",
       "         topic_7   topic_8   topic_9  ...  topic_110  topic_111  topic_112  \\\n",
       "33425   0.000269  0.000269  0.000269  ...   0.000269   0.000269   0.000269   \n",
       "174759  0.000309  0.000309  0.000309  ...   0.000309   0.000309   0.000309   \n",
       "111229  0.000333  0.000333  0.000333  ...   0.000333   0.000333   0.000333   \n",
       "209101  0.000397  0.000397  0.000397  ...   0.000397   0.000397   0.000397   \n",
       "109643  0.000758  0.000758  0.106666  ...   0.000758   0.000758   0.000758   \n",
       "...          ...       ...       ...  ...        ...        ...        ...   \n",
       "59447   0.000096  0.029456  0.000096  ...   0.000096   0.000096   0.000096   \n",
       "97003   0.000362  0.000362  0.000362  ...   0.000362   0.000362   0.000362   \n",
       "64465   0.000181  0.065573  0.000181  ...   0.000181   0.029271   0.000181   \n",
       "91943   0.016370  0.004398  0.006995  ...   0.088138   0.000023   0.016547   \n",
       "141385  0.000309  0.059193  0.000309  ...   0.000309   0.000309   0.000309   \n",
       "\n",
       "        topic_113  topic_114  topic_115  topic_116  topic_117  topic_118  \\\n",
       "33425    0.000269   0.000269   0.000269   0.000269   0.000269   0.000269   \n",
       "174759   0.000309   0.000309   0.000309   0.000309   0.000309   0.000309   \n",
       "111229   0.000333   0.000333   0.000333   0.000333   0.000333   0.048490   \n",
       "209101   0.000397   0.000397   0.000397   0.000397   0.000397   0.000397   \n",
       "109643   0.000758   0.000758   0.000758   0.000758   0.000758   0.135033   \n",
       "...           ...        ...        ...        ...        ...        ...   \n",
       "59447    0.000096   0.024203   0.000096   0.000096   0.018131   0.012412   \n",
       "97003    0.000362   0.000362   0.053188   0.000362   0.000362   0.000362   \n",
       "64465    0.000181   0.000181   0.000181   0.000181   0.057708   0.000181   \n",
       "91943    0.012009   0.012541   0.013296   0.000023   0.030237   0.008304   \n",
       "141385   0.000309   0.000309   0.000309   0.201535   0.000309   0.000309   \n",
       "\n",
       "        topic_119  \n",
       "33425    0.000269  \n",
       "174759   0.000309  \n",
       "111229   0.000333  \n",
       "209101   0.000397  \n",
       "109643   0.000758  \n",
       "...           ...  \n",
       "59447    0.000096  \n",
       "97003    0.000362  \n",
       "64465    0.000181  \n",
       "91943    0.000023  \n",
       "141385   0.000309  \n",
       "\n",
       "[50000 rows x 120 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "willing-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "therapeutic-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_model_and_dump(num_topics=100):\n",
    "\n",
    "    print('Running ' + str(num_topics) + ' topic LDA')\n",
    "    \n",
    "    # # Train the model on the corpus.\n",
    "    start = time.time()\n",
    "    lda = LdaModel(corpus,  id2word=id2word, num_topics=num_topics)\n",
    "    stop = time.time()\n",
    "    print(\"training the model took \" +str(stop-start) + \" seconds to complete\")\n",
    "\n",
    "    start = time.time()\n",
    "    #Stole the below code from\n",
    "    #https://stackoverflow.com/questions/46574720/python-gensim-lda-add-the-topic-to-the-document-after-getting-the-topics\n",
    "    all_topics = lda.get_document_topics(corpus, minimum_probability=0.0)\n",
    "    all_topics_csr = gensim.matutils.corpus2csc(all_topics)\n",
    "    all_topics_numpy = all_topics_csr.T.toarray()\n",
    "    all_topics_df = pd.DataFrame(all_topics_numpy)\n",
    "    stop = time.time()\n",
    "    print(\"pulling the topcs took \" +str(stop-start) + \" seconds to complete\")\n",
    "\n",
    "    #Let's rename the topic columns so that we can identify them later on\n",
    "    all_topics_df.index = df.index\n",
    "    for each in all_topics_df.columns:\n",
    "        all_topics_df['topic_' + str(each)] = all_topics_df[each]\n",
    "        all_topics_df.drop(columns=[each], inplace=True)\n",
    "\n",
    "    start = time.time()\n",
    "    return all_topics_df    \n",
    "    #pickle.dump(all_topics_df, open(\"full_df_\" +str(num_topics) + \"_topics\", 'wb'))\n",
    "    stop = time.time()\n",
    "    print('Succesfully saved ' + str(num_topics) + ' dataframe' + 'it took ' +str(stop-start) + ' seoconds to save')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-shoulder",
   "metadata": {},
   "source": [
    "# Vectorizing the corpus\n",
    "#Creating a df for each degree of n-gram vector, for easy organization\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-hindu",
   "metadata": {},
   "source": [
    "Because of the limited dimensionality we have to play with we've opted to build our own vocabulary for the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "unsigned-solomon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n",
      "100\n"
     ]
    }
   ],
   "source": [
    "lexicon = slicer(50, 50, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "canadian-abraham",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "federal-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer_Agent_mono = TfidfVectorizer(preprocessor=None, stop_words=stopwords, max_features=num_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "objective-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectored_no_stops_mono = Vectorizer_Agent_mono.fit_transform(df.text.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "administrative-starter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363219, 10)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectored_no_stops_mono.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "legitimate-prisoner",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_only_list= [0,0,100,50,50,25]\n",
    "neg_only_list= [0,0,100,50,50,50]\n",
    "high_ratio_list = [0,100,0,100,200,225]\n",
    "topic_list= [600,500,400,400,300,300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "strange-genome",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_space = {'pos_only' : pos_only_list,\n",
    "             'neg_only':neg_only_list,\n",
    "             'high_ratio':high_ratio_list,\n",
    "            'topics':topic_list}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-briefs",
   "metadata": {},
   "source": [
    "# Aggregating our X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "promotional-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_and_topics = [vectored_no_stops_mono, topic_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "professional-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hstack(mono_and_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-eagle",
   "metadata": {},
   "source": [
    "## Bringing in and extracting our Y column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "extended-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('C:/Users/Prathmun/Documents/Springboard Jupyter/Sentiment-Capstone/data/food/test.csv')\n",
    "train = pd.read_csv('C:/Users/Prathmun/Documents/Springboard Jupyter/Sentiment-Capstone/data/food/train.csv')\n",
    "frames = [train, test]\n",
    "df = pd.concat(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "combined-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = resample(df,\n",
    "                         replace=False,\n",
    "                         n_samples=int(sample_size),\n",
    "                        random_state=12,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "tight-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "chemical-corner",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-94fccadcb29d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue_counts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnormalized\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'y' is not defined"
     ]
    }
   ],
   "source": [
    "y.value_counts(normalized=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "every-northern",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "express-snapshot",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "preceding-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "proof-subsection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37500, 10)\n",
      "(37500,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-authentication",
   "metadata": {},
   "source": [
    "# Save our Xs and ys to pass onto the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fifteen-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_train, open(\"X_train\", 'wb'))\n",
    "pickle.dump(X_test, open(\"X_test\", 'wb'))\n",
    "pickle.dump(y_train, open(\"y_train\", 'wb'))\n",
    "pickle.dump(y_test, open(\"y_test\", 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-utilization",
   "metadata": {},
   "source": [
    "# Process to map modeling times based on dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "textile-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timing_buddy(model_to_fit, dictionary_string, dicto):\n",
    "    funky_time_start = time.time()\n",
    "    model_to_fit.fit(X_train, np.ravel(y_train))\n",
    "    funky_time_stop = time.time()\n",
    "    funky_train_time = funky_time_stop - funky_time_start\n",
    "    dicto[dictionary_string]['training_time'] = funky_train_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "horizontal-belarus",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confused_buddy(model_to_confuse, dictionary_string,dicto):\n",
    "    confuse = confusion_matrix(y_test, model_to_confuse.predict(X_test))\n",
    "    dicto[dictionary_string]['confuse'] = confuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "regional-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_calculator(confuse):\n",
    "    recall = 0\n",
    "    tp = confuse[1][1]\n",
    "    fn = confuse[1][0]\n",
    "    if tp > 0 or tp ==1:\n",
    "        recall = tp / (tp+fn)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "operating-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_hyper_grad_boosting_model(dicto, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    def GradBoost_hyper(n_estimators, learning_rate, max_leaf_nodes):\n",
    "        grady_the_boosted = GradientBoostingClassifier(learning_rate=learning_rate,  n_estimators=int(n_estimators),\n",
    "                                                      max_leaf_nodes=int(max_leaf_nodes))\n",
    "        grady_the_boosted.fit(X_train, np.ravel(y_train))\n",
    "        confuse = confusion_matrix(y_test, grady_the_boosted.predict(X_test))\n",
    "        recall = recall_calculator(confuse)\n",
    "        return recall\n",
    "    \n",
    "    ###\n",
    "    GradBoost_params = {'learning_rate' : (.1,2),\n",
    "                    'n_estimators':(10,500),\n",
    "                    'max_leaf_nodes':(3,50)}\n",
    "#     ###\n",
    "    \n",
    "#     optimizer = BayesianOptimization(\n",
    "#     GradBoost_hyper,\n",
    "#     pbounds=GradBoost_params,\n",
    "#     verbose=1)\n",
    "    \n",
    "#     ######\n",
    "    \n",
    "#     print('Gradient Boosting')\n",
    "#     optimizer.maximize(init_points=3, n_iter=4)\n",
    "#     grad_boost_params= optimizer.max\n",
    "#     grad_boost_params = grad_boost_params['params']\n",
    "    \n",
    "    ###\n",
    "    grady_the_boosted = GradientBoostingClassifier(learning_rate=1.0 ,\n",
    "                                                   n_estimators=100 ,\n",
    "                                                   max_leaf_nodes=4 )\n",
    "    timing_buddy(grady_the_boosted, 'GradientBoostingClassifier', dicto)\n",
    "    confused_buddy(grady_the_boosted, 'GradientBoostingClassifier', dicto)\n",
    "    dicto['GradientBoostingClassifier']['ROC_AUC_Score'] = roc_auc_score(y_test, grady_the_boosted.predict_proba(X_test)[:, 1])\n",
    "    dicto['GradientBoostingClassifier']['Hyper_Params'] = grad_boost_params\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "temporal-tuesday",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_maker(grid_dict)\n",
    "\n",
    "for i in range[len(grid_dict['pos_only'])]\n",
    "    num_topcs = grid_dict['topics'][i]    \n",
    "    all_topics_df = topic_model_and_dump(num_topics)\n",
    "        \n",
    "    topic_vector = scipy.sparse.csr_matrix(all_topics_df.values)\n",
    "    \n",
    "    neg_only = grid_dict['neg_only'][i]\n",
    "    high_ratio = grid_dict['high_ratio'][i]\n",
    "    lexicon = slicer(pos_only, neg_only, high_ratio)\n",
    "\n",
    "    Vectorizer_Agent_mono = TfidfVectorizer(preprocessor=None, stop_words=stopwords, max_features=num_feats, vocabulary=lexicon)\n",
    "\n",
    "    mono_and_topics = [vectored_no_stops_mono, topic_vector]\n",
    "    \n",
    "    X = hstack(mono_and_topics)\n",
    "    y= df.Y\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    \n",
    "    name +'_pos_' + str(pos_only) +'_neg_' + str('neg_only') + '_ratio_'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "rocky-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chonky_model_aggregator(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    #Models to explore\n",
    "    model_set = ['GradientBoostingClassifier']\n",
    "    model_stats = {}\n",
    "    for each in model_set:\n",
    "        model_stats[each] ={'confuse' : [], 'training_time' : 0, 'ROC_AUC_Score': 0, 'Hyper_Params': {}}\n",
    "    bayes_hyper_grad_boosting_model(model_stats, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "\n",
    "    return model_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "peaceful-fifth",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "russian-shopping",
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulator = pd.DataFrame(columns=['confuse', 'training_time', 'hyper_params', 'ROC_AUC_Score'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "driving-timothy",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "accompanied-composer",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>confuse</th>\n",
       "      <th>training_time</th>\n",
       "      <th>hyper_params</th>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [confuse, training_time, hyper_params, ROC_AUC_Score]\n",
       "Index: []"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accumulator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "religious-marriage",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = {4:5,3:2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "scheduled-induction",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4: 5, 3: 2}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "mexican-banking",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "acquired-sellers",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "adapted-protocol",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363219,)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fossil-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dimensional_mapper(features):\n",
    "    details_dict = {}\n",
    "    time_to_execute= 0\n",
    "\n",
    "    #Make the X vector with the proper number of features\n",
    "        ##y should stay the same, so we're not rebuilding it each iteration\n",
    "\n",
    "    Vectorizer_Agent_mono = TfidfVectorizer(preprocessor=None, stop_words=stopwords, max_features=int(features))\n",
    "    X = Vectorizer_Agent_mono.fit_transform(df.text.values) \n",
    "    print(X.shape)\n",
    "\n",
    "    #train/test split it\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "    #run the model\n",
    "\n",
    "    stats = chonky_model_aggregator(X_train, y_train, X_test, y_test)\n",
    "\n",
    "    #record it's stats\n",
    "\n",
    "    time_to_execute = stats['GradientBoostingClassifier']['training_time']\n",
    "    details_dict[features] = stats['GradientBoostingClassifier']\n",
    "\n",
    "    #increase the number of features\n",
    "    print('the last iteration took ' + str(time_to_execute) + ' seconds to train')\n",
    "    print('FEATURES = ' + str(features))\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    if time_to_execute < 600:\n",
    "        return -600 + time_to_execute\n",
    "    if time_to_execture > 600:\n",
    "        return 600 - time_to_execute\n",
    "    if time_to_execute == 600:\n",
    "        return 100\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "guilty-franchise",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensional_params = {'features': (10,5000)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "civilian-ethernet",
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensional_optimizer = BayesianOptimization(\n",
    "    dimensional_mapper,\n",
    "    pbounds=dimensional_params,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "fluid-jesus",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | features  |\n",
      "-------------------------------------\n",
      "(50000, 2029)\n",
      "Gradient Boosting\n",
      "|   iter    |  target   | learni... | max_le... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.9699  \u001b[0m | \u001b[95m 1.675   \u001b[0m | \u001b[95m 21.58   \u001b[0m | \u001b[95m 482.5   \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.97    \u001b[0m | \u001b[95m 1.841   \u001b[0m | \u001b[95m 18.69   \u001b[0m | \u001b[95m 164.3   \u001b[0m |\n",
      "| \u001b[95m 8       \u001b[0m | \u001b[95m 0.9771  \u001b[0m | \u001b[95m 1.818   \u001b[0m | \u001b[95m 4.346   \u001b[0m | \u001b[95m 276.0   \u001b[0m |\n",
      "| \u001b[95m 15      \u001b[0m | \u001b[95m 0.9791  \u001b[0m | \u001b[95m 1.372   \u001b[0m | \u001b[95m 27.02   \u001b[0m | \u001b[95m 99.05   \u001b[0m |\n",
      "=============================================================\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 2029 features, but DecisionTreeRegressor is expecting 44 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    190\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 191\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_cache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0m_hashable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: (2029.8752319202822,)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-33ea97448abe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdimensional_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmaximize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minit_points\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mgrad_boost_params\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mgrad_boost_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_boost_params\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'params'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mmaximize\u001b[1;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[0;32m    183\u001b[0m                 \u001b[0miteration\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_probe\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlazy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    186\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bounds_transformer\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\bayes_opt\\bayesian_optimization.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params, lazy)\u001b[0m\n\u001b[0;32m    114\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 116\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprobe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    117\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mEvents\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOPTIMIZATION_STEP\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\bayes_opt\\target_space.py\u001b[0m in \u001b[0;36mprobe\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    192\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    193\u001b[0m             \u001b[0mparams\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 194\u001b[1;33m             \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    195\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    196\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-76-70fefafef2ee>\u001b[0m in \u001b[0;36mdimensional_mapper\u001b[1;34m(features)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;31m#run the model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchonky_model_aggregator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m#record it's stats\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-56-07260cd219eb>\u001b[0m in \u001b[0;36mchonky_model_aggregator\u001b[1;34m(X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmodel_set\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mmodel_stats\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0meach\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'confuse'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'training_time'\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ROC_AUC_Score'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Hyper_Params'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mbayes_hyper_grad_boosting_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_stats\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-55-170d7052fad0>\u001b[0m in \u001b[0;36mbayes_hyper_grad_boosting_model\u001b[1;34m(dicto, X_train, y_train, X_test, y_test)\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[0mtiming_buddy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrady_the_boosted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GradientBoostingClassifier'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdicto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[0mconfused_buddy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrady_the_boosted\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'GradientBoostingClassifier'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdicto\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m     \u001b[0mdicto\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'GradientBoostingClassifier'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ROC_AUC_Score'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrady_the_boosted\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m     \u001b[0mdicto\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'GradientBoostingClassifier'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Hyper_Params'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrad_boost_params\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mpredict_proba\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1235\u001b[0m             \u001b[0mclasses\u001b[0m \u001b[0mcorresponds\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthat\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mattribute\u001b[0m \u001b[1;33m:\u001b[0m\u001b[0mterm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1236\u001b[0m         \"\"\"\n\u001b[1;32m-> 1237\u001b[1;33m         \u001b[0mraw_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1238\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1239\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_prediction_to_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_predictions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36mdecision_function\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m   1142\u001b[0m         \"\"\"\n\u001b[0;32m   1143\u001b[0m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"C\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'csr'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1144\u001b[1;33m         \u001b[0mraw_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1145\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1146\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mraw_predictions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_raw_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    622\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_raw_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    623\u001b[0m         \u001b[1;34m\"\"\"Return the sum of the trees raw predictions (+ init estimator).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 624\u001b[1;33m         \u001b[0mraw_predictions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_raw_predict_init\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    625\u001b[0m         predict_stages(self.estimators_, X, self.learning_rate,\n\u001b[0;32m    626\u001b[0m                        raw_predictions)\n",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\ensemble\\_gb.py\u001b[0m in \u001b[0;36m_raw_predict_init\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    608\u001b[0m         \u001b[1;34m\"\"\"Check input and compute raw predictions of the init estimator.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_validate_X_predict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m             raise ValueError(\"X.shape[1] should be {0:d}, not {1:d}.\".format(\n",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36m_validate_X_predict\u001b[1;34m(self, X, check_input)\u001b[0m\n\u001b[0;32m    405\u001b[0m         \u001b[1;34m\"\"\"Validate the training data on predict (probabilities).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    406\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 407\u001b[1;33m             X = self._validate_data(X, dtype=DTYPE, accept_sparse=\"csr\",\n\u001b[0m\u001b[0;32m    408\u001b[0m                                     reset=False)\n\u001b[0;32m    409\u001b[0m             if issparse(X) and (X.indices.dtype != np.intc or\n",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ensure_2d'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 437\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_n_features\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    364\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 365\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    366\u001b[0m                 \u001b[1;34mf\"X has {n_features} features, but {self.__class__.__name__} \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    367\u001b[0m                 f\"is expecting {self.n_features_in_} features as input.\")\n",
      "\u001b[1;31mValueError\u001b[0m: X has 2029 features, but DecisionTreeRegressor is expecting 44 features as input."
     ]
    }
   ],
   "source": [
    "dimensional_optimizer.maximize(init_points=4, n_iter=20)\n",
    "grad_boost_params= optimizer.min\n",
    "grad_boost_params = grad_boost_params['params']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-update",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "convenient-scotland",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(363219, 700)\n",
      "Gradient Boosting\n",
      "|   iter    |  target   | learni... | max_le... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.967   \u001b[0m | \u001b[95m 0.5597  \u001b[0m | \u001b[95m 34.93   \u001b[0m | \u001b[95m 378.3   \u001b[0m |\n",
      "| \u001b[95m 6       \u001b[0m | \u001b[95m 0.9852  \u001b[0m | \u001b[95m 1.9     \u001b[0m | \u001b[95m 4.72    \u001b[0m | \u001b[95m 98.7    \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.9875  \u001b[0m | \u001b[95m 1.905   \u001b[0m | \u001b[95m 26.04   \u001b[0m | \u001b[95m 267.5   \u001b[0m |\n",
      "=============================================================\n",
      "the last iteration took 768.8696720600128 seconds to train\n",
      "FEATURES = 700\n",
      " \n",
      " \n",
      " \n"
     ]
    }
   ],
   "source": [
    "features = 700\n",
    "time_to_execute= 0\n",
    "details_dict = {}\n",
    "\n",
    "while 600 > time_to_execute:\n",
    "    \n",
    "    #Make the X vector with the proper number of features\n",
    "        ##y should stay the same, so we're not rebuilding it each iteration\n",
    "        \n",
    "    Vectorizer_Agent_mono = TfidfVectorizer(preprocessor=None, stop_words=stopwords, max_features=features)\n",
    "    X = Vectorizer_Agent_mono.fit_transform(df.text.values) \n",
    "    print(X.shape)\n",
    "    \n",
    "    #train/test split it\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "    \n",
    "    #run the model\n",
    "    \n",
    "    stats = chonky_model_aggregator(X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    #record it's stats\n",
    "    \n",
    "    time_to_execute = stats['GradientBoostingClassifier']['training_time']\n",
    "    details_dict[features] = stats['GradientBoostingClassifier']\n",
    "    \n",
    "    #increase the number of features\n",
    "    print('the last iteration took ' + str(time_to_execute) + ' seconds to train')\n",
    "    print('FEATURES = ' + str(features))\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    print(' ')\n",
    "    features += 100\n",
    "    if len(details_dict) > 4:\n",
    "        pickle.dump(details_dict, open(\"dimensionality_training_data_max_\" +str(features) + '_features', 'wb'))\n",
    "        details_dict = {}\n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "reserved-texture",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{700: {'confuse': array([[  826, 13523],\n",
       "         [  959, 75497]], dtype=int64),\n",
       "  'training_time': 768.8696720600128,\n",
       "  'ROC_AUC_Score': 0.579698770470151,\n",
       "  'Hyper_Params': {'learning_rate': 1.9045136766473334,\n",
       "   'max_leaf_nodes': 26.036159694472282,\n",
       "   'n_estimators': 267.48774390256347}}}"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "geological-threat",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>600</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Hyper_Params</th>\n",
       "      <td>{'learning_rate': 1.8157960344591315, 'max_lea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ROC_AUC_Score</th>\n",
       "      <td>0.527477</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                             600\n",
       "Hyper_Params   {'learning_rate': 1.8157960344591315, 'max_lea...\n",
       "ROC_AUC_Score                                           0.527477"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "details_df = pd.DataFrame(details_dict)\n",
    "details_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "agreed-advice",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'training_time'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-109-9c081fdf9725>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdetails_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_time\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\users\\prathmun\\appdata\\local\\programs\\python\\python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5458\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_info_axis\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_can_hold_identifiers_and_holds_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5459\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5460\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'training_time'"
     ]
    }
   ],
   "source": [
    "details_df.training_time.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "departmental-module",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(details_df, open(\"modeling_training_curve_700\", 'wb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adolescent-essence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
