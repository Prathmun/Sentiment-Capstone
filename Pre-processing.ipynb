{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "honey-cardiff",
   "metadata": {},
   "source": [
    "# Imports and basic set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "original-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the begining, ther is time\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "quiet-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quality of life\n",
    "#Some things we do generate a lot of warnings, and it just becomes clutter.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#I like it when my notebook helps me out\n",
    "%config IPCompleter.greedy=True\n",
    "#Sometiems you just need to print pretty\n",
    "from pprint import pprint\n",
    "#Lots of these operations take many minutes to complete. So it behooves us to pickle the outputs and just unpickle them each time we re-open the notebook\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "entire-vehicle",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "import pandas as pd\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-patio",
   "metadata": {},
   "source": [
    "Data is from: https://www.kaggle.com/datasets/toygarr/datasets-for-natural-language-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-yahoo",
   "metadata": {},
   "source": [
    "Dataset is from a collection of sentiment datasets, but we just want to play with the food one for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-brake",
   "metadata": {},
   "source": [
    "\"ctweet, stweet, food\" datasets are positive or negative analysis (sentiment) -> 0 negative -> 1 positive (ctweet has neutral 0, 1, 2)\n",
    "\n",
    "we're assuming that any code we write to deal with this food data set will be extensible later by simply adding the rest of the data if we so choose. At the moment it's simply faster to work with a smaller subset of the data as we design the pipeline and it's displays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-length",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "north-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import ngrams\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-fancy",
   "metadata": {},
   "source": [
    "## Corpus and Vectoring tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "final-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.cluster import DBSCAN\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-worse",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "thirty-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('C:/Users/Prathmun/Documents/Springboard Jupyter/Sentiment-Capstone/data/food/test.csv')\n",
    "train = pd.read_csv('C:/Users/Prathmun/Documents/Springboard Jupyter/Sentiment-Capstone/data/food/train.csv')\n",
    "frames = [train, test]\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-little",
   "metadata": {},
   "source": [
    "# Processing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-limitation",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "noted-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df.text.apply(word_tokenize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "crucial-creation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Y</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i was put off at first by the green powder but...</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, was, put, off, at, first, by, the, green, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these ginger chews are too good to be true i t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[these, ginger, chews, are, too, good, to, be,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Y  \\\n",
       "0  i was put off at first by the green powder but...  1   \n",
       "1  these ginger chews are too good to be true i t...  1   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [i, was, put, off, at, first, by, the, green, ...  \n",
       "1  [these, ginger, chews, are, too, good, to, be,...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-cancellation",
   "metadata": {},
   "source": [
    "### Remove Stop Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "abroad-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "intensive-berkeley",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['nostops'] = df.tokenized.apply(lambda  x: [item for item in x if item not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "healthy-introduction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Y</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>nostops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i was put off at first by the green powder but...</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, was, put, off, at, first, by, the, green, ...</td>\n",
       "      <td>[put, first, green, powder, bad, little, grain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these ginger chews are too good to be true i t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[these, ginger, chews, are, too, good, to, be,...</td>\n",
       "      <td>[ginger, chews, good, true, try, limit, one, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Y  \\\n",
       "0  i was put off at first by the green powder but...  1   \n",
       "1  these ginger chews are too good to be true i t...  1   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [i, was, put, off, at, first, by, the, green, ...   \n",
       "1  [these, ginger, chews, are, too, good, to, be,...   \n",
       "\n",
       "                                             nostops  \n",
       "0  [put, first, green, powder, bad, little, grain...  \n",
       "1  [ginger, chews, good, true, try, limit, one, d...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-prospect",
   "metadata": {},
   "source": [
    "### Stemming \n",
    "Might do lemmatization later, but stemming is simpler and more reliable and we're looking to get something working before we refine it. Lemmatization is a step we can experiment with in our refinement stages\n",
    "\n",
    "We chose snowball because it's an older, and stable stemmer that incorporates improvements from the older stemmer algorithm Porter's real world experience\n",
    "Snowball docs: https://www.nltk.org/api/nltk.stem.snowball.html\n",
    "Article that informed our decision to choose Snowball: https://towardsdatascience.com/stemming-lemmatization-what-ba782b7c0bd8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-victory",
   "metadata": {},
   "source": [
    "It looks as if stemming can increase recall, even in short texts but can also cause problems. We're going to move forward with stemmed words for now, but again can return to this if we seek optimization tasks down the line\n",
    "Source: https://stackoverflow.com/questions/47219389/compute-word-n-grams-on-original-text-or-after-lemma-stemming-process#:~:text=Computing%20word%20n%2Dgrams%20after,you%20want%20to%20do%20it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "terminal-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "df['stemmed'] = df.nostops.apply(lambda x: [stemmer.stem(item) for item in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "physical-milton",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [put, first, green, powder, bad, littl, graini...\n",
       "1    [ginger, chew, good, true, tri, limit, one, da...\n",
       "Name: stemmed, dtype: object"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stemmed.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "knowing-design",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(363219, 5)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "artistic-bhutan",
   "metadata": {},
   "source": [
    "# Building Limited Vocabularies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "documented-habitat",
   "metadata": {},
   "outputs": [],
   "source": [
    "nostops_text_neg = \" \".join(\" \".join(listo) for listo in df.nostops[df['Y'] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "stuck-stadium",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_list = nostops_text_neg.split(' ')\n",
    "neg_set = set(neg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "thick-crash",
   "metadata": {},
   "outputs": [],
   "source": [
    "nostops_text_pos = \" \".join(\" \".join(listo) for listo in df.nostops[df['Y'] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "micro-concord",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_list = nostops_text_pos.split(' ')\n",
    "pos_set = set(pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "productive-wilderness",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg list len = 2334177\n",
      "neg set len = 46886\n"
     ]
    }
   ],
   "source": [
    "print('neg list len = ' + str(len(neg_list)))\n",
    "print('neg set len = ' + str(len(neg_set))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "treated-coordinate",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos list len = 11470406\n",
      "pos set len = 104877\n"
     ]
    }
   ],
   "source": [
    "print('pos list len = ' + str(len(pos_list)))\n",
    "print('pos set len = ' + str(len(pos_set))) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "smooth-thumbnail",
   "metadata": {},
   "source": [
    "## Ratio DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-collins",
   "metadata": {},
   "source": [
    "## Extracting Negative Only words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "rocky-editor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11892"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neg_only_set = neg_set - pos_set\n",
    "len(neg_only_set )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "material-watts",
   "metadata": {},
   "source": [
    "### Neg only Count DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aging-appraisal",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11892"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building up the count for the neg only df so that we can order them to slice, so we can add them to the lexicon we're going to feed into the vocabulary of the tokenizer\n",
    "\n",
    "neg_count = {}\n",
    "\n",
    "for word in neg_list:\n",
    "    if word in neg_only_set:\n",
    "        if word not in neg_count.keys():\n",
    "            \n",
    "            neg_count[word] = 0\n",
    "            \n",
    "        neg_count[word] += 1\n",
    "        \n",
    "neg_count_df = pd.DataFrame(neg_count, index=[0])\n",
    "                            \n",
    "neg_only_count_df = neg_count_df.transpose()\n",
    "\n",
    "len(neg_only_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "funny-cutting",
   "metadata": {},
   "source": [
    "## Extracting Positive Only Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "trying-fisher",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69883"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_only_set =  pos_set - neg_set \n",
    "len(pos_only_set )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rotary-albania",
   "metadata": {},
   "source": [
    "### Pos Only Count Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "stretch-permission",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69883"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Building up the count for the neg only df so that we can order them to slice, so we can add them to the lexicon we're going to feed into the vocabulary of the tokenizer\n",
    "\n",
    "pos_count = {}\n",
    "\n",
    "for word in pos_list:\n",
    "    if word in pos_only_set:\n",
    "        if word not in pos_count.keys():\n",
    "            \n",
    "            pos_count[word] = 0\n",
    "            \n",
    "        pos_count[word] += 1\n",
    "        \n",
    "pos_count_df = pd.DataFrame(pos_count, index=[0])\n",
    "                            \n",
    "pos_only_count_df = pos_count_df.transpose()\n",
    "\n",
    "len(pos_only_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "round-tractor",
   "metadata": {},
   "source": [
    "### Extracting word ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "narrative-wheat",
   "metadata": {},
   "source": [
    "Using set logic to snag words that only appear in both positive and negative polarity documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "adjacent-explanation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34994"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shared_set = neg_set.intersection(pos_set)\n",
    "len(shared_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "collaborative-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_df =  pd.DataFrame(shared_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-environment",
   "metadata": {},
   "source": [
    "### Counting the negative appearences of our set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "duplicate-memory",
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_count = {}\n",
    "\n",
    "for word in neg_list:\n",
    "    if word in shared_set:\n",
    "        if word not in neg_count.keys():\n",
    "            \n",
    "            neg_count[word] = 0\n",
    "            \n",
    "        neg_count[word] += 1\n",
    "        \n",
    "neg_count_df = pd.DataFrame(neg_count, index=[0])\n",
    "                            \n",
    "neg_count_df = neg_count_df.transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sixth-antenna",
   "metadata": {},
   "source": [
    "### Counting the positive appearences of our set of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "plain-juice",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_count = {}\n",
    "\n",
    "for word in pos_list:\n",
    "    if word in shared_set:\n",
    "        if word not in pos_count.keys():\n",
    "            \n",
    "            pos_count[word] = 0\n",
    "            \n",
    "        pos_count[word] += 1\n",
    "\n",
    "pos_count_df = pd.DataFrame(pos_count, index=[0])\n",
    "\n",
    "pos_count_df = pos_count_df.transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "intensive-savage",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Stapling the pos/neg counts to the ratio df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "forbidden-conversion",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_df['neg_count'] = neg_count_df[0]\n",
    "ratio_df['pos_count'] = pos_count_df[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "numeric-egyptian",
   "metadata": {},
   "source": [
    "### Getting ratiod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "digital-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio_df['ratio'] = ratio_df[['neg_count','pos_count']].mean(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dress-placement",
   "metadata": {},
   "source": [
    "# Function to slice the lexicons appropriately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "mineral-energy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slicer(pos_only, neg_only, high_ratio):\n",
    "    \n",
    "    #slice pos\n",
    "    pos_slice = list(pos_only_count_df.sort_values(by=[0], ascending=False).head(pos_only).index)\n",
    "    print(len(pos_slice))\n",
    "    #slice neg\n",
    "    neg_slice = list(neg_only_count_df.sort_values(by=[0], ascending=False).head(neg_only).index)\n",
    "    print(len(neg_slice))\n",
    "    #slice high_ratio\n",
    "    ratio_slice = list(ratio_df.sort_values(ascending=False, by='ratio').head(high_ratio+1).index)[1:]\n",
    "    print(len(ratio_slice))\n",
    "    \n",
    "    \n",
    "    #Join slices into one big list\n",
    "    sliced_lexicons = [pos_slice,neg_slice,ratio_slice]\n",
    "    sliced_lexicon = []\n",
    "    for lexicon in sliced_lexicons:\n",
    "        for word in lexicon:\n",
    "            sliced_lexicon.append(word)\n",
    "        \n",
    "    #Return list\n",
    "    return sliced_lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-timber",
   "metadata": {},
   "source": [
    "# <span style=\"color:purple\"> Clustering and Secondary EDA/Feature Engineering</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-river",
   "metadata": {},
   "source": [
    "We played briefly with using DBSCAN, but it didn't converge at all, it gave us 360k categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-victorian",
   "metadata": {},
   "source": [
    "Thinking that instead of conventional clustering we're going to play with latent dirichlet allocation because it'll do a similar thing as K-means, but also provide us with a lot more information about the composition of those clusters.\n",
    "Original Paper: https://ai.stanford.edu/~ang/papers/nips01-lda.pdf\n",
    "Tutorial we used to help us get going: https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "Docs for the gensim module we're doing the lda with: https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "european-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_texts = list(df.stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "nuclear-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes a dictionary mapping to unique IDs\n",
    "id2word = Dictionary(stemmed_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "insured-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes a bag of words version of the Gensim Dictionary\n",
    "corpus = [id2word.doc2bow(text) for text in stemmed_texts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-termination",
   "metadata": {},
   "source": [
    "# Feature Engineering with LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-thought",
   "metadata": {},
   "source": [
    "## Topic Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-hungarian",
   "metadata": {},
   "source": [
    "Documentation we're going to need to use to get at document relationships to topics: https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-taste",
   "metadata": {},
   "source": [
    "Relevant stack overflow\n",
    "https://stackoverflow.com/questions/43357247/get-document-topics-and-get-term-topics-in-gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "broke-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "willing-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "therapeutic-prompt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_model_and_dump(num_topics=100):\n",
    "\n",
    "    print('Running ' + str(num_topics) + ' topic LDA')\n",
    "    \n",
    "    # # Train the model on the corpus.\n",
    "    start = time.time()\n",
    "    lda = LdaModel(corpus,  id2word=id2word, num_topics=num_topics)\n",
    "    stop = time.time()\n",
    "    print(\"training the model took \" +str(stop-start) + \" seconds to complete\")\n",
    "\n",
    "    start = time.time()\n",
    "    #Stole the below code from\n",
    "    #https://stackoverflow.com/questions/46574720/python-gensim-lda-add-the-topic-to-the-document-after-getting-the-topics\n",
    "    all_topics = lda.get_document_topics(corpus, minimum_probability=0.0)\n",
    "    all_topics_csr = gensim.matutils.corpus2csc(all_topics)\n",
    "    all_topics_numpy = all_topics_csr.T.toarray()\n",
    "    all_topics_df = pd.DataFrame(all_topics_numpy)\n",
    "    stop = time.time()\n",
    "    print(\"pulling the topcs took \" +str(stop-start) + \" seconds to complete\")\n",
    "\n",
    "    #Let's rename the topic columns so that we can identify them later on\n",
    "    all_topics_df.index = df.index\n",
    "    for each in all_topics_df.columns:\n",
    "        all_topics_df['topic_' + str(each)] = all_topics_df[each]\n",
    "        all_topics_df.drop(columns=[each], inplace=True)\n",
    "\n",
    "    start = time.time()\n",
    "    #return all_topics_df    \n",
    "    topic_vector = scipy.sparse.csr_matrix(all_topics_df.values)\n",
    "    pickle.dump(all_topics_df, open(\"topic_vector\" +str(num_topics) + \"_topics\", 'wb'))\n",
    "    stop = time.time()\n",
    "    print('Succesfully saved ' + str(num_topics) + ' dataframe' + 'it took ' +str(stop-start) + ' seoconds to save')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "electronic-jenny",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for value in [5,10,25,50,100,150,200,300,400,500,600]:\n",
    "#      topic_model_and_dump(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ordered-utilization",
   "metadata": {},
   "source": [
    "# Process to map modeling times based on dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "regional-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_calculator(confuse):\n",
    "    recall = 0\n",
    "    tp = confuse[1][1]\n",
    "    fn = confuse[1][0]\n",
    "    if tp > 0 or tp ==1:\n",
    "        recall = tp / (tp+fn)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "operating-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_boosting_model(dicto, X_train, y_train, X_test, y_test):\n",
    "    x_y = [X_train, y_train, X_test, y_test]\n",
    "    grady_the_boosted = GradientBoostingClassifier(learning_rate=1.0 ,\n",
    "                                                   n_estimators=50 ,\n",
    "                                                   max_leaf_nodes=4 )\n",
    "    \n",
    "    \n",
    "    #timing_buddy(grady_the_boosted, 'GradientBoostingClassifier', dicto, x_y)\n",
    "    #Timethe training\n",
    "    funky_time_start = time.time()\n",
    "    grady_the_boosted.fit(X_train, y_train)\n",
    "    #     model_to_fit.fit(X_train, np.ravel(y_train))\n",
    "    funky_time_stop = time.time()\n",
    "    funky_train_time = funky_time_stop - funky_time_start\n",
    "    dicto['GradientBoostingClassifier']['training_time'] = funky_train_time\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    confuse = confusion_matrix(y_test, grady_the_boosted.predict(X_test))\n",
    "    dicto['GradientBoostingClassifier']['confuse'] = confuse\n",
    "\n",
    "    dicto['GradientBoostingClassifier']['ROC_AUC_Score'] = roc_auc_score(y_test, grady_the_boosted.predict_proba(X_test)[:, 1])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "rocky-sharp",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_aggregator(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    #Models to explore\n",
    "    model_set = ['GradientBoostingClassifier']\n",
    "    model_stats = {}\n",
    "    for each in model_set:\n",
    "        model_stats[each] ={'confuse' : [], 'training_time' : 0, 'ROC_AUC_Score': 0}   \n",
    "    grad_boosting_model(model_stats, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "\n",
    "    return model_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "secure-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_maker(grid_dict):\n",
    "\n",
    "    for i in range(len(grid_dict['pos_only'])):\n",
    "        num_topics = grid_dict['topics'][i]    \n",
    "        \n",
    "        print('pulling the ' +str(grid_dict['topics'][i]) + ' topic vector')\n",
    "        start = time.time()\n",
    "        topic_vector = pickle.load(open('topic_vector' +str(grid_dict['topics'][i]) + \"_topics\", 'rb'))\n",
    "        topic_vector = scipy.sparse.csr_matrix(topic_vector.values)\n",
    "        stop = time.time()\n",
    "        print(topic_vector.shape)\n",
    "        print('Succesfully loaded the topic vector it took ' +str(int(stop-start)) + ' seoconds to load')\n",
    "        #Priming the quantity of depth we wish to pull from in our individual lexicons\n",
    "        \n",
    "        pos_only = grid_dict['pos_only'][i]\n",
    "        neg_only = grid_dict['neg_only'][i]\n",
    "        high_ratio = grid_dict['high_ratio'][i]\n",
    "        \n",
    "        #compile the actual lexicon we're going to pass to the vectorizer\n",
    "        start = time.time()\n",
    "        print('pos, neg, ratio quantities')\n",
    "        lexicon = slicer(pos_only, neg_only, high_ratio)\n",
    "        \n",
    "        #tokenize/vectorize the text with the vocabulary we produced\n",
    "        Vectorizer_Agent_mono = TfidfVectorizer(preprocessor=None, stop_words=stopwords, vocabulary=lexicon)\n",
    "        vectored_no_stops_mono = Vectorizer_Agent_mono.fit_transform(df.text.values) \n",
    "        stop = time.time()\n",
    "        print('Succesfully loaded the token vector it took ' +str(int(stop-start)) + ' seoconds to load')\n",
    "        \n",
    "        \n",
    "        #Join together our topic and token vectors\n",
    "        #(This one was tricky, set it up originally to use the sci.py hstack, but when moving the funciton I switched over to the\n",
    "        #numpy hstack on accident, which doesn't handle sparse matrices easily. Spent like two days bamboozled here)\n",
    "        start= time.time()\n",
    "        print('Joining vectors')\n",
    "        mono_and_topics = (vectored_no_stops_mono, topic_vector)\n",
    "        X = hstack(mono_and_topics)\n",
    "        stop = time.time()\n",
    "\n",
    "        print('Succesfully joined the vectors it took ' +str(int(stop-start)) + ' seoconds to join')\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        #NOthin' special needs to happen here. I just wanted my Y getting built within the function so I wouldn't have to track it down\n",
    "        #if something breaks\n",
    "        y= df.Y\n",
    "        \n",
    "        #ye-old splits\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "        \n",
    "        #Using my prebuilt function to train/test/score the model\n",
    "        print('Training model')\n",
    "        start= time.time()\n",
    "\n",
    "        model_stats= model_aggregator(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        stop= time.time()\n",
    "        print('Succesfully trained/tested the model it took ' +str(int(stop-start)) + ' seoconds to train/test')\n",
    "\n",
    "        print('Saving model')\n",
    "        start= time.time()\n",
    "        pickle.dump( model_stats,\n",
    "                    open(\n",
    "                        'model_stats' +'_pos_' + str(pos_only) +'_neg_' + str(neg_only) + '_ratio_' + str(high_ratio) + '_topcis_' +str(num_topics) +'.p',\n",
    "                        \"wb\" ) )\n",
    "        stop= time.time()\n",
    "        print('Succesfully saved the model results, it took ' +str(int(stop-start)) + ' seoconds to save')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "worth-liberia",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_only_list= [100,50,100,50,50,50,25]\n",
    "neg_only_list= [50,100,100,100,50,50,50]\n",
    "high_ratio_list = [400,400,300,300,300,250,275]\n",
    "topic_list= [50,50,100,150,200,250,250]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "willing-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_only_list= [50]\n",
    "# neg_only_list= [100]\n",
    "# high_ratio_list = [600-160]\n",
    "# topic_list= [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "steady-enterprise",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_space = {'pos_only' : pos_only_list,\n",
    "             'neg_only':neg_only_list,\n",
    "             'high_ratio':high_ratio_list,\n",
    "            'topics':topic_list}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sensitive-search",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "white-visibility",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling the 50 topic vector\n",
      "(363219, 50)\n",
      "Succesfully loaded the topic vector it took 1 seoconds to load\n",
      "pos, neg, ratio quantities\n",
      "100\n",
      "50\n",
      "400\n",
      "Succesfully loaded the token vector it took 16 seoconds to load\n",
      "Joining vectors\n",
      "Succesfully joined the vectors it took 0 seoconds to join\n",
      "Training model\n",
      "Succesfully trained/tested the model it took 307 seoconds to train/test\n",
      "Saving model\n",
      "Succesfully saved the model results, it took 0 seoconds to save\n",
      "pulling the 50 topic vector\n",
      "(363219, 50)\n",
      "Succesfully loaded the topic vector it took 1 seoconds to load\n",
      "pos, neg, ratio quantities\n",
      "50\n",
      "100\n",
      "400\n",
      "Succesfully loaded the token vector it took 16 seoconds to load\n",
      "Joining vectors\n",
      "Succesfully joined the vectors it took 0 seoconds to join\n",
      "Training model\n",
      "Succesfully trained/tested the model it took 301 seoconds to train/test\n",
      "Saving model\n",
      "Succesfully saved the model results, it took 0 seoconds to save\n",
      "pulling the 100 topic vector\n",
      "(363219, 100)\n",
      "Succesfully loaded the topic vector it took 2 seoconds to load\n",
      "pos, neg, ratio quantities\n",
      "100\n",
      "100\n",
      "300\n",
      "Succesfully loaded the token vector it took 16 seoconds to load\n",
      "Joining vectors\n",
      "Succesfully joined the vectors it took 1 seoconds to join\n",
      "Training model\n",
      "Succesfully trained/tested the model it took 584 seoconds to train/test\n",
      "Saving model\n",
      "Succesfully saved the model results, it took 0 seoconds to save\n",
      "pulling the 150 topic vector\n",
      "(363219, 150)\n",
      "Succesfully loaded the topic vector it took 4 seoconds to load\n",
      "pos, neg, ratio quantities\n",
      "50\n",
      "100\n",
      "300\n",
      "Succesfully loaded the token vector it took 16 seoconds to load\n",
      "Joining vectors\n",
      "Succesfully joined the vectors it took 1 seoconds to join\n",
      "Training model\n",
      "Succesfully trained/tested the model it took 888 seoconds to train/test\n",
      "Saving model\n",
      "Succesfully saved the model results, it took 0 seoconds to save\n",
      "pulling the 200 topic vector\n",
      "(363219, 200)\n",
      "Succesfully loaded the topic vector it took 5 seoconds to load\n",
      "pos, neg, ratio quantities\n",
      "50\n",
      "50\n",
      "300\n",
      "Succesfully loaded the token vector it took 26 seoconds to load\n",
      "Joining vectors\n",
      "Succesfully joined the vectors it took 3 seoconds to join\n",
      "Training model\n",
      "Succesfully trained/tested the model it took 1159 seoconds to train/test\n",
      "Saving model\n",
      "Succesfully saved the model results, it took 0 seoconds to save\n",
      "pulling the 250 topic vector\n",
      "(363219, 250)\n",
      "Succesfully loaded the topic vector it took 6 seoconds to load\n",
      "pos, neg, ratio quantities\n",
      "50\n",
      "50\n",
      "250\n",
      "Succesfully loaded the token vector it took 17 seoconds to load\n",
      "Joining vectors\n",
      "Succesfully joined the vectors it took 4 seoconds to join\n",
      "Training model\n",
      "Succesfully trained/tested the model it took 1438 seoconds to train/test\n",
      "Saving model\n",
      "Succesfully saved the model results, it took 0 seoconds to save\n",
      "pulling the 250 topic vector\n",
      "(363219, 250)\n",
      "Succesfully loaded the topic vector it took 6 seoconds to load\n",
      "pos, neg, ratio quantities\n",
      "25\n",
      "50\n",
      "275\n",
      "Succesfully loaded the token vector it took 25 seoconds to load\n",
      "Joining vectors\n",
      "Succesfully joined the vectors it took 3 seoconds to join\n",
      "Training model\n",
      "Succesfully trained/tested the model it took 1435 seoconds to train/test\n",
      "Saving model\n",
      "Succesfully saved the model results, it took 0 seoconds to save\n"
     ]
    }
   ],
   "source": [
    "grid_maker(grid_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "dying-yellow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AFTER THE FACT DUMMY MAKER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "organic-capacity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "expected-photographer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classy_dummy(dicto, X_train, y_train, X_test, y_test):\n",
    "    print('dumbo')\n",
    "    dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "    \n",
    "    \n",
    "    funky_time_start = time.time()\n",
    "    dummy_clf.fit(X_train, y_train)\n",
    "    #     model_to_fit.fit(X_train, np.ravel(y_train))\n",
    "    funky_time_stop = time.time()\n",
    "    funky_train_time = funky_time_stop - funky_time_start\n",
    "    \n",
    "    dicto['dummy']['training_time'] = funky_train_time\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    confuse = confusion_matrix(y_test, dummy_clf.predict(X_test))\n",
    "    dicto['dummy']['confuse'] = confuse\n",
    "\n",
    "    \n",
    "    dicto['dummy']['ROC_AUC_Score'] = roc_auc_score(y_test, dummy_clf.predict_proba(X_test)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "latin-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_aggregator(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    #Models to explore\n",
    "    model_set = ['dummy']\n",
    "    model_stats = {}\n",
    "    for each in model_set:\n",
    "        model_stats[each] ={'confuse' : [], 'training_time' : 0, 'ROC_AUC_Score': 0}   \n",
    "    classy_dummy(model_stats, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "\n",
    "    return model_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "persistent-sculpture",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_maker(grid_dict):\n",
    "\n",
    "    for i in range(len(grid_dict['pos_only'])):\n",
    "        num_topics = grid_dict['topics'][i]    \n",
    "        \n",
    "        print('pulling the ' +str(grid_dict['topics'][i]) + ' topic vector')\n",
    "        start = time.time()\n",
    "        topic_vector = pickle.load(open('topic_vector' +str(grid_dict['topics'][i]) + \"_topics\", 'rb'))\n",
    "        topic_vector = scipy.sparse.csr_matrix(topic_vector.values)\n",
    "        stop = time.time()\n",
    "        print(topic_vector.shape)\n",
    "        print('Succesfully loaded the topic vector it took ' +str(int(stop-start)) + ' seoconds to load')\n",
    "        #Priming the quantity of depth we wish to pull from in our individual lexicons\n",
    "        \n",
    "        pos_only = grid_dict['pos_only'][i]\n",
    "        neg_only = grid_dict['neg_only'][i]\n",
    "        high_ratio = grid_dict['high_ratio'][i]\n",
    "        \n",
    "        #compile the actual lexicon we're going to pass to the vectorizer\n",
    "        start = time.time()\n",
    "        print('pos, neg, ratio quantities')\n",
    "        lexicon = slicer(pos_only, neg_only, high_ratio)\n",
    "        \n",
    "        #tokenize/vectorize the text with the vocabulary we produced\n",
    "        Vectorizer_Agent_mono = TfidfVectorizer(preprocessor=None, stop_words=stopwords, vocabulary=lexicon)\n",
    "        vectored_no_stops_mono = Vectorizer_Agent_mono.fit_transform(df.text.values) \n",
    "        stop = time.time()\n",
    "        print('Succesfully loaded the token vector it took ' +str(int(stop-start)) + ' seoconds to load')\n",
    "        \n",
    "        \n",
    "        #Join together our topic and token vectors\n",
    "        #(This one was tricky, set it up originally to use the sci.py hstack, but when moving the funciton I switched over to the\n",
    "        #numpy hstack on accident, which doesn't handle sparse matrices easily. Spent like two days bamboozled here)\n",
    "        start= time.time()\n",
    "        print('Joining vectors')\n",
    "        mono_and_topics = (vectored_no_stops_mono, topic_vector)\n",
    "        X = hstack(mono_and_topics)\n",
    "        stop = time.time()\n",
    "\n",
    "        print('Succesfully joined the vectors it took ' +str(int(stop-start)) + ' seoconds to join')\n",
    "        \n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "        #NOthin' special needs to happen here. I just wanted my Y getting built within the function so I wouldn't have to track it down\n",
    "        #if something breaks\n",
    "        y= df.Y\n",
    "        \n",
    "        #ye-old splits\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "        \n",
    "        #Using my prebuilt function to train/test/score the model\n",
    "        print('Training model')\n",
    "        start= time.time()\n",
    "\n",
    "        model_stats= model_aggregator(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        stop= time.time()\n",
    "        print('Succesfully trained/tested the model it took ' +str(int(stop-start)) + ' seoconds to train/test')\n",
    "\n",
    "        print('Saving model')\n",
    "        start= time.time()\n",
    "        pickle.dump( model_stats,\n",
    "                    open(\n",
    "                        'dummy_stats' +'_pos_' + str(pos_only) +'_neg_' + str(neg_only) + '_ratio_' + str(high_ratio) + '_topcis_' +str(num_topics) +'.p',\n",
    "                        \"wb\" ) )\n",
    "        stop= time.time()\n",
    "        print('Succesfully saved the model results, it took ' +str(int(stop-start)) + ' seoconds to save')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "alternate-harbor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pulling the 50 topic vector\n",
      "(363219, 50)\n",
      "Succesfully loaded the topic vector it took 1 seoconds to load\n",
      "pos, neg, ratio quantities\n",
      "100\n",
      "50\n",
      "400\n",
      "Succesfully loaded the token vector it took 16 seoconds to load\n",
      "Joining vectors\n",
      "Succesfully joined the vectors it took 0 seoconds to join\n",
      "Training model\n",
      "dumbo\n",
      "Succesfully trained/tested the model it took 0 seoconds to train/test\n",
      "Saving model\n",
      "Succesfully saved the model results, it took 0 seoconds to save\n",
      "pulling the 50 topic vector\n",
      "(363219, 50)\n",
      "Succesfully loaded the topic vector it took 1 seoconds to load\n",
      "pos, neg, ratio quantities\n",
      "50\n",
      "100\n",
      "400\n",
      "Succesfully loaded the token vector it took 16 seoconds to load\n",
      "Joining vectors\n",
      "Succesfully joined the vectors it took 0 seoconds to join\n",
      "Training model\n",
      "dumbo\n",
      "Succesfully trained/tested the model it took 0 seoconds to train/test\n",
      "Saving model\n",
      "Succesfully saved the model results, it took 0 seoconds to save\n",
      "pulling the 100 topic vector\n",
      "(363219, 100)\n",
      "Succesfully loaded the topic vector it took 3 seoconds to load\n",
      "pos, neg, ratio quantities\n",
      "100\n",
      "100\n",
      "300\n",
      "Succesfully loaded the token vector it took 16 seoconds to load\n",
      "Joining vectors\n",
      "Succesfully joined the vectors it took 1 seoconds to join\n",
      "Training model\n",
      "dumbo\n",
      "Succesfully trained/tested the model it took 0 seoconds to train/test\n",
      "Saving model\n",
      "Succesfully saved the model results, it took 0 seoconds to save\n",
      "pulling the 150 topic vector\n",
      "(363219, 150)\n",
      "Succesfully loaded the topic vector it took 6 seoconds to load\n",
      "pos, neg, ratio quantities\n",
      "50\n",
      "100\n",
      "300\n",
      "Succesfully loaded the token vector it took 17 seoconds to load\n",
      "Joining vectors\n",
      "Succesfully joined the vectors it took 1 seoconds to join\n",
      "Training model\n",
      "dumbo\n",
      "Succesfully trained/tested the model it took 0 seoconds to train/test\n",
      "Saving model\n",
      "Succesfully saved the model results, it took 0 seoconds to save\n",
      "pulling the 200 topic vector\n",
      "(363219, 200)\n",
      "Succesfully loaded the topic vector it took 9 seoconds to load\n",
      "pos, neg, ratio quantities\n",
      "50\n",
      "50\n",
      "300\n",
      "Succesfully loaded the token vector it took 18 seoconds to load\n",
      "Joining vectors\n",
      "Succesfully joined the vectors it took 2 seoconds to join\n",
      "Training model\n",
      "dumbo\n",
      "Succesfully trained/tested the model it took 0 seoconds to train/test\n",
      "Saving model\n",
      "Succesfully saved the model results, it took 0 seoconds to save\n",
      "pulling the 250 topic vector\n",
      "250\n",
      "Succesfully loaded the token vector it took 37 seoconds to load\n",
      "Joining vectors\n",
      "Succesfully joined the vectors it took 73 seoconds to join\n",
      "Training model\n",
      "dumbo\n",
      "Succesfully trained/tested the model it took 7 seoconds to train/test\n",
      "Saving model\n",
      "Succesfully saved the model results, it took 0 seoconds to save\n",
      "pulling the 250 topic vector\n",
      "(363219, 250)\n",
      "Succesfully loaded the topic vector it took 401 seoconds to load\n",
      "pos, neg, ratio quantities\n",
      "25\n",
      "50\n",
      "275\n",
      "Succesfully loaded the token vector it took 263 seoconds to load\n",
      "Joining vectors\n",
      "Succesfully joined the vectors it took 20 seoconds to join\n",
      "Training model\n",
      "dumbo\n",
      "Succesfully trained/tested the model it took 5 seoconds to train/test\n",
      "Saving model\n",
      "Succesfully saved the model results, it took 0 seoconds to save\n"
     ]
    }
   ],
   "source": [
    "grid_maker(grid_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "velvet-catalog",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
