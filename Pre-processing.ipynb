{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "honey-cardiff",
   "metadata": {},
   "source": [
    "# Imports and basic set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "original-snowboard",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the begining, ther is time\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "quiet-burner",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Quality of life\n",
    "#Some things we do generate a lot of warnings, and it just becomes clutter.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#I like it when my notebook helps me out\n",
    "%config IPCompleter.greedy=True\n",
    "#Sometiems you just need to print pretty\n",
    "from pprint import pprint\n",
    "#Lots of these operations take many minutes to complete. So it behooves us to pickle the outputs and just unpickle them each time we re-open the notebook\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "clear-patio",
   "metadata": {},
   "source": [
    "Data is from: https://www.kaggle.com/datasets/toygarr/datasets-for-natural-language-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-yahoo",
   "metadata": {},
   "source": [
    "Dataset is from a collection of sentiment datasets, but we just want to play with the food one for now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-brake",
   "metadata": {},
   "source": [
    "\"ctweet, stweet, food\" datasets are positive or negative analysis (sentiment) -> 0 negative -> 1 positive (ctweet has neutral 0, 1, 2)\n",
    "\n",
    "we're assuming that any code we write to deal with this food data set will be extensible later by simply adding the rest of the data if we so choose. At the moment it's simply faster to work with a smaller subset of the data as we design the pipeline and it's displays."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comic-length",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "north-corpus",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk import ngrams\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binding-fancy",
   "metadata": {},
   "source": [
    "## Corpus and Vectoring tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "final-estimate",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.cluster import DBSCAN\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "pleased-database",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forward-worse",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "thirty-european",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('C:/Users/Prathmun/Documents/Springboard Jupyter/Sentiment-Capstone/data/food/test.csv')\n",
    "train = pd.read_csv('C:/Users/Prathmun/Documents/Springboard Jupyter/Sentiment-Capstone/data/food/train.csv')\n",
    "frames = [train, test]\n",
    "df = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "integrated-pontiac",
   "metadata": {},
   "source": [
    "Nobs to play with, want 'em nearby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "medical-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 50000\n",
    "num_feats =10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "studied-carnival",
   "metadata": {},
   "source": [
    "# Sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unavailable-intensity",
   "metadata": {},
   "source": [
    "Here we can set up the code to downsample for speed's sake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "elementary-discussion",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = df.head(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "conditional-tenant",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 50000\n",
    "df = resample(df,\n",
    "                         replace=False,\n",
    "                         n_samples=sample_size,\n",
    "                        random_state=12,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "alternate-christian",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "considerable-franklin",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    42229\n",
       "0     7771\n",
       "Name: Y, dtype: int64"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.Y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incident-little",
   "metadata": {},
   "source": [
    "# Processing the text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-limitation",
   "metadata": {},
   "source": [
    "### Tokenize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "noted-significance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokenized'] = df.text.apply(word_tokenize)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "crucial-creation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Y</th>\n",
       "      <th>tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i was put off at first by the green powder but...</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, was, put, off, at, first, by, the, green, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these ginger chews are too good to be true i t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[these, ginger, chews, are, too, good, to, be,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Y  \\\n",
       "0  i was put off at first by the green powder but...  1   \n",
       "1  these ginger chews are too good to be true i t...  1   \n",
       "\n",
       "                                           tokenized  \n",
       "0  [i, was, put, off, at, first, by, the, green, ...  \n",
       "1  [these, ginger, chews, are, too, good, to, be,...  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vietnamese-cancellation",
   "metadata": {},
   "source": [
    "### Remove Stop Words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "abroad-growth",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "intensive-berkeley",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['nostops'] = df.tokenized.apply(lambda  x: [item for item in x if item not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "healthy-introduction",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>Y</th>\n",
       "      <th>tokenized</th>\n",
       "      <th>nostops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i was put off at first by the green powder but...</td>\n",
       "      <td>1</td>\n",
       "      <td>[i, was, put, off, at, first, by, the, green, ...</td>\n",
       "      <td>[put, first, green, powder, bad, little, grain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>these ginger chews are too good to be true i t...</td>\n",
       "      <td>1</td>\n",
       "      <td>[these, ginger, chews, are, too, good, to, be,...</td>\n",
       "      <td>[ginger, chews, good, true, try, limit, one, d...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  Y  \\\n",
       "0  i was put off at first by the green powder but...  1   \n",
       "1  these ginger chews are too good to be true i t...  1   \n",
       "\n",
       "                                           tokenized  \\\n",
       "0  [i, was, put, off, at, first, by, the, green, ...   \n",
       "1  [these, ginger, chews, are, too, good, to, be,...   \n",
       "\n",
       "                                             nostops  \n",
       "0  [put, first, green, powder, bad, little, grain...  \n",
       "1  [ginger, chews, good, true, try, limit, one, d...  "
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "lesser-prospect",
   "metadata": {},
   "source": [
    "### Stemming \n",
    "Might do lemmatization later, but stemming is simpler and more reliable and we're looking to get something working before we refine it. Lemmatization is a step we can experiment with in our refinement stages\n",
    "\n",
    "We chose snowball because it's an older, and stable stemmer that incorporates improvements from the older stemmer algorithm Porter's real world experience\n",
    "Snowball docs: https://www.nltk.org/api/nltk.stem.snowball.html\n",
    "Article that informed our decision to choose Snowball: https://towardsdatascience.com/stemming-lemmatization-what-ba782b7c0bd8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-victory",
   "metadata": {},
   "source": [
    "It looks as if stemming can increase recall, even in short texts but can also cause problems. We're going to move forward with stemmed words for now, but again can return to this if we seek optimization tasks down the line\n",
    "Source: https://stackoverflow.com/questions/47219389/compute-word-n-grams-on-original-text-or-after-lemma-stemming-process#:~:text=Computing%20word%20n%2Dgrams%20after,you%20want%20to%20do%20it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "terminal-copying",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "df['stemmed'] = df.nostops.apply(lambda x: [stemmer.stem(item) for item in x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "physical-milton",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33425     [like, low, carb, browni, top, notch, sever, b...\n",
       "174759    [use, feed, dog, read, ingredi, despit, promot...\n",
       "Name: stemmed, dtype: object"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stemmed.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "legal-shoulder",
   "metadata": {},
   "source": [
    "# Vectorizing the corpus\n",
    "#Creating a df for each degree of n-gram vector, for easy organization\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "statewide-hindu",
   "metadata": {},
   "source": [
    "### Count vectorizer Paramters to play with"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stone-discrimination",
   "metadata": {},
   "source": [
    "max_dffloat in range [0.0, 1.0] or int, default=1.0\n",
    "When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words). If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None.\n",
    "\n",
    "min_dffloat in range [0.0, 1.0] or int, default=1\n",
    "When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold. This value is also called cut-off in the literature. If float, the parameter represents a proportion of documents, integer absolute counts. This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-cosmetic",
   "metadata": {},
   "source": [
    "max_featuresint, default=None\n",
    "If not None, build a vocabulary that only consider the top max_features ordered by term frequency across the corpus.\n",
    "\n",
    "This parameter is ignored if vocabulary is not None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "federal-presentation",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer_Agent_mono = TfidfVectorizer(preprocessor=None, stop_words=stopwords, max_features=num_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "objective-cardiff",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectored_no_stops_mono = Vectorizer_Agent_mono.fit_transform(df.text.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "administrative-starter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectored_no_stops_mono.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "electronic-squad",
   "metadata": {},
   "source": [
    "### Bigram Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "parliamentary-courage",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer_Agent_bi = TfidfVectorizer(preprocessor=None, stop_words=stopwords, ngram_range=(1,2), max_features=num_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "peaceful-wallet",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectored_no_stops_bi = Vectorizer_Agent_bi.fit_transform(df.text.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "reverse-paste",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectored_no_stops_bi.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "biblical-triple",
   "metadata": {},
   "source": [
    "### Trigram Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "scientific-fantasy",
   "metadata": {},
   "outputs": [],
   "source": [
    "Vectorizer_Agent_tri = TfidfVectorizer(preprocessor=None, stop_words=stopwords, ngram_range=(1,3), max_features=num_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "recent-shelf",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectored_no_stops_tri = Vectorizer_Agent_tri.fit_transform(df.text.values) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "seven-marijuana",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 10)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectored_no_stops_tri.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alive-timber",
   "metadata": {},
   "source": [
    "# <span style=\"color:purple\"> Clustering and Secondary EDA/Feature Engineering</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-river",
   "metadata": {},
   "source": [
    "We played briefly with using DBSCAN, but it didn't converge at all, it gave us 360k categories"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-victorian",
   "metadata": {},
   "source": [
    "Thinking that instead of conventional clustering we're going to play with latent dirichlet allocation because it'll do a similar thing as K-means, but also provide us with a lot more information about the composition of those clusters.\n",
    "Original Paper: https://ai.stanford.edu/~ang/papers/nips01-lda.pdf\n",
    "Tutorial we used to help us get going: https://towardsdatascience.com/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0\n",
    "Docs for the gensim module we're sdoing the lda with: https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "european-grove",
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmed_texts = list(df.stemmed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "nuclear-department",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes a dictionary mapping to unique IDs\n",
    "id2word = Dictionary(stemmed_texts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "insured-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes a bag of words version of the Gensim Dictionary\n",
    "corpus = [id2word.doc2bow(text) for text in stemmed_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "persistent-butter",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this took 26.939680099487305 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "# # Train the model on the corpus.\n",
    "start = time.time()\n",
    "num_topics = 10\n",
    "lda = LdaModel(corpus,  id2word=id2word, num_topics=num_topics)\n",
    "stop = time.time()\n",
    "print(\"this took \" +str(stop-start) + \" seconds to complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "editorial-termination",
   "metadata": {},
   "source": [
    "# Feature Engineering with LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-thought",
   "metadata": {},
   "source": [
    "## Topic Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-hungarian",
   "metadata": {},
   "source": [
    "Documentation we're going to need to use to get at document relationships to topics: https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constitutional-taste",
   "metadata": {},
   "source": [
    "Relevant stack overflow\n",
    "https://stackoverflow.com/questions/43357247/get-document-topics-and-get-term-topics-in-gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "broke-potential",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "liquid-commercial",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this took 21.358879566192627 seconds to complete\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "#Stole the below code from\n",
    "#https://stackoverflow.com/questions/46574720/python-gensim-lda-add-the-topic-to-the-document-after-getting-the-topics\n",
    "all_topics = lda.get_document_topics(corpus, minimum_probability=0.0)\n",
    "all_topics_csr = gensim.matutils.corpus2csc(all_topics)\n",
    "all_topics_numpy = all_topics_csr.T.toarray()\n",
    "all_topics_df = pd.DataFrame(all_topics_numpy)\n",
    "stop = time.time()\n",
    "print(\"this took \" +str(stop-start) + \" seconds to complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "gorgeous-paraguay",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is a feature of designing our code with downsampled data. Just need the index's to be consistent.\n",
    "all_topics_df.index = df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "vanilla-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's rename the topic columns so that we can identify them later on\n",
    "for each in all_topics_df.columns:\n",
    "    all_topics_df['topic_' + str(each)] = all_topics_df[each]\n",
    "    all_topics_df.drop(columns=[each], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "working-gallery",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>topic_9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>33425</th>\n",
       "      <td>0.003228</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>0.275443</td>\n",
       "      <td>0.117731</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>0.189158</td>\n",
       "      <td>0.398302</td>\n",
       "      <td>0.003228</td>\n",
       "      <td>0.003228</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174759</th>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.966654</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.003705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111229</th>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.963987</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.004001</td>\n",
       "      <td>0.004002</td>\n",
       "      <td>0.004002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209101</th>\n",
       "      <td>0.004763</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.004763</td>\n",
       "      <td>0.260149</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.382232</td>\n",
       "      <td>0.324274</td>\n",
       "      <td>0.004764</td>\n",
       "      <td>0.004764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>109643</th>\n",
       "      <td>0.698323</td>\n",
       "      <td>0.009096</td>\n",
       "      <td>0.009094</td>\n",
       "      <td>0.009095</td>\n",
       "      <td>0.228922</td>\n",
       "      <td>0.009095</td>\n",
       "      <td>0.009093</td>\n",
       "      <td>0.009095</td>\n",
       "      <td>0.009093</td>\n",
       "      <td>0.009094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59447</th>\n",
       "      <td>0.297327</td>\n",
       "      <td>0.224064</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.040608</td>\n",
       "      <td>0.001113</td>\n",
       "      <td>0.213282</td>\n",
       "      <td>0.054235</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>0.032557</td>\n",
       "      <td>0.134589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97003</th>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.570731</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.004350</td>\n",
       "      <td>0.004349</td>\n",
       "      <td>0.300656</td>\n",
       "      <td>0.098169</td>\n",
       "      <td>0.004349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64465</th>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.272367</td>\n",
       "      <td>0.174221</td>\n",
       "      <td>0.538511</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.002129</td>\n",
       "      <td>0.002129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91943</th>\n",
       "      <td>0.067618</td>\n",
       "      <td>0.049209</td>\n",
       "      <td>0.000273</td>\n",
       "      <td>0.024400</td>\n",
       "      <td>0.139197</td>\n",
       "      <td>0.201719</td>\n",
       "      <td>0.406452</td>\n",
       "      <td>0.077397</td>\n",
       "      <td>0.033461</td>\n",
       "      <td>0.000273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141385</th>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.697087</td>\n",
       "      <td>0.273276</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.003704</td>\n",
       "      <td>0.003705</td>\n",
       "      <td>0.003705</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         topic_0   topic_1   topic_2   topic_3   topic_4   topic_5   topic_6  \\\n",
       "33425   0.003228  0.003228  0.003228  0.275443  0.117731  0.003228  0.189158   \n",
       "174759  0.003705  0.003705  0.003705  0.003705  0.966654  0.003705  0.003705   \n",
       "111229  0.004001  0.004002  0.004001  0.004001  0.963987  0.004001  0.004002   \n",
       "209101  0.004763  0.004764  0.004763  0.260149  0.004764  0.004764  0.382232   \n",
       "109643  0.698323  0.009096  0.009094  0.009095  0.228922  0.009095  0.009093   \n",
       "...          ...       ...       ...       ...       ...       ...       ...   \n",
       "59447   0.297327  0.224064  0.001112  0.040608  0.001113  0.213282  0.054235   \n",
       "97003   0.004349  0.570731  0.004349  0.004349  0.004349  0.004350  0.004349   \n",
       "64465   0.002129  0.002129  0.002129  0.272367  0.174221  0.538511  0.002129   \n",
       "91943   0.067618  0.049209  0.000273  0.024400  0.139197  0.201719  0.406452   \n",
       "141385  0.003705  0.697087  0.273276  0.003705  0.003705  0.003705  0.003705   \n",
       "\n",
       "         topic_7   topic_8   topic_9  \n",
       "33425   0.398302  0.003228  0.003228  \n",
       "174759  0.003705  0.003705  0.003705  \n",
       "111229  0.004001  0.004002  0.004002  \n",
       "209101  0.324274  0.004764  0.004764  \n",
       "109643  0.009095  0.009093  0.009094  \n",
       "...          ...       ...       ...  \n",
       "59447   0.001112  0.032557  0.134589  \n",
       "97003   0.300656  0.098169  0.004349  \n",
       "64465   0.002129  0.002129  0.002129  \n",
       "91943   0.077397  0.033461  0.000273  \n",
       "141385  0.003704  0.003705  0.003705  \n",
       "\n",
       "[50000 rows x 10 columns]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_topics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "willing-quebec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "corporate-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_vector = scipy.sparse.csr_matrix(all_topics_df.values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "illegal-briefs",
   "metadata": {},
   "source": [
    "# Aggregating our X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "assigned-ballet",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_stack = [vectored_no_stops_mono, vectored_no_stops_bi, vectored_no_stops_tri]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "temporal-lesson",
   "metadata": {},
   "outputs": [],
   "source": [
    "bi_vector_stack = [vectored_no_stops_mono, vectored_no_stops_bi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "expanded-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_vector_stack = [vectored_no_stops_mono]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "promotional-brazil",
   "metadata": {},
   "outputs": [],
   "source": [
    "mono_and_topics = [vectored_no_stops_mono, topic_vector]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "professional-maria",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = hstack(mono_vector_stack)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecological-eagle",
   "metadata": {},
   "source": [
    "## Bringing in and extracting our Y column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "extended-negotiation",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('C:/Users/Prathmun/Documents/Springboard Jupyter/Sentiment-Capstone/data/food/test.csv')\n",
    "train = pd.read_csv('C:/Users/Prathmun/Documents/Springboard Jupyter/Sentiment-Capstone/data/food/train.csv')\n",
    "frames = [train, test]\n",
    "df = pd.concat(frames)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "combined-salvation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = resample(df,\n",
    "                         replace=False,\n",
    "                         n_samples=int(sample_size),\n",
    "                        random_state=12,\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "tight-romance",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-corner",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.value_counts(normalized=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "express-snapshot",
   "metadata": {},
   "source": [
    "# Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "preceding-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "proof-subsection",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37500, 10)\n",
      "(37500,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brown-authentication",
   "metadata": {},
   "source": [
    "# Save our Xs and ys to pass onto the next notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "fifteen-belief",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(X_train, open(\"X_train\", 'wb'))\n",
    "pickle.dump(X_test, open(\"X_test\", 'wb'))\n",
    "pickle.dump(y_train, open(\"y_train\", 'wb'))\n",
    "pickle.dump(y_test, open(\"y_test\", 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-emergency",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
