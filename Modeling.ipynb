{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "occupational-beauty",
   "metadata": {},
   "source": [
    "# Module and Data Importation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "natural-mortality",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-party",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "southeast-international",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-rubber",
   "metadata": {},
   "source": [
    "## Modeling Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "noble-harrison",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.utils import resample\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fleet-arrest",
   "metadata": {},
   "source": [
    "# Load our Xs and Ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "proprietary-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pickle.load(open(\"X_train\", 'rb'))\n",
    "X_test = pickle.load(open(\"X_test\", 'rb'))\n",
    "y_train = pickle.load(open(\"y_train\", 'rb'))\n",
    "y_test = pickle.load(open(\"y_test\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-accommodation",
   "metadata": {},
   "source": [
    "### Helper Funcitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "close-stuart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def timing_buddy(model_to_fit, dictionary_string, dicto):\n",
    "    funky_time_start = time.time()\n",
    "    model_to_fit.fit(X_train, np.ravel(y_train))\n",
    "    funky_time_stop = time.time()\n",
    "    funky_train_time = funky_time_stop - funky_time_start\n",
    "    dicto[dictionary_string]['training_time'] = funky_train_time\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "standing-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "def confused_buddy(model_to_confuse, dictionary_string,dicto):\n",
    "    confuse = confusion_matrix(y_test, model_to_confuse.predict(X_test))\n",
    "    dicto[dictionary_string]['confuse'] = confuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "preceding-queensland",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall_calculator(confuse):\n",
    "    recall = 0\n",
    "    tp = confuse[1][1]\n",
    "    fn = confuse[1][0]\n",
    "    if tp > 0 or tp ==1:\n",
    "        recall = tp / (tp+fn)\n",
    "    return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "vocational-stupid",
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision_calculator(confuse):\n",
    "    precision = 0\n",
    "    print(confuse)\n",
    "    tp = confuse[1][1]\n",
    "    fp = confuse[0][1]\n",
    "    if tp > 0 or tp ==1:\n",
    "        precision = tp / (tp+fp)\n",
    "    return precision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-cherry",
   "metadata": {},
   "source": [
    "## Dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "harmful-samuel",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classy_dummy(dicto, X_train, y_train, X_test, y_test):\n",
    "    print('dumbo')\n",
    "    dummy_clf = DummyClassifier(strategy=\"stratified\")\n",
    "    timing_buddy(dummy_clf, 'dummy', dicto)\n",
    "    confused_buddy(dummy_clf, 'dummy', dicto)\n",
    "    confused_buddy(dummy_clf, 'dummy', dicto)\n",
    "    dicto['dummy']['ROC_AUC_Score'] = roc_auc_score(y_test, dummy_clf.predict_proba(X_test)[:, 1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interior-extension",
   "metadata": {},
   "source": [
    "## SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "opposed-celtic",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SVCmodeler(dicto, X_train, y_train, X_test, y_test):\n",
    "    print('Support Vector Classification')\n",
    "    SupportVectorClassifier = SVC()\n",
    "    timing_buddy(SupportVectorClassifier, 'SVC', dicto)\n",
    "    confused_buddy(SupportVectorClassifier, 'SVC', dicto)\n",
    "    dicto['SVC']['ROC_AUC_Score'] = roc_auc_score(y_test, SupportVectorClassifier.decision_function(X_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reduced-freeze",
   "metadata": {},
   "source": [
    "## Stochastic Gradiant Descent "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "matched-accused",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_sgd_model(dicto, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    def SGDHyperParameterer(loss_penalty_list):\n",
    "        SGD_clf = SGDClassifier(loss = loss_penalty_list[0], penalty=loss_penalty_list[1])\n",
    "        SGD_clf.fit(X_train, np.ravel(y_train))\n",
    "        confuse = confusion_matrix(y_test, SGD_clf.predict(X_test))\n",
    "        recall = recall_calculator(confuse)\n",
    "        return recall\n",
    "    \n",
    "    \n",
    "    losses= ['modified_huber', 'log']\n",
    "    penalties = ['l2', 'l1', 'elasticnet']\n",
    "    param_grid = {}\n",
    "    count = 0\n",
    "    for each_loss in losses:\n",
    "        for each_penalty in penalties:\n",
    "            param_grid[str(each_loss) + '-' + str(each_penalty)] = [each_loss, each_penalty]\n",
    "            count+=1\n",
    "\n",
    "    \n",
    "    ###\n",
    "    SGD_Hyper_param_recall_scores = {}\n",
    "    for key, value in param_grid.items():\n",
    "        SGD_Hyper_param_recall_scores[key] = SGDHyperParameterer(value)\n",
    "\n",
    "    ###\n",
    "    \n",
    "    print('Stochastic Gradiant Descent')\n",
    "    best_recall = 0\n",
    "    penalty = ''\n",
    "    loss = ''\n",
    "    for key, value in SGD_Hyper_param_recall_scores.items():\n",
    "        if value > best_recall:\n",
    "            best_recall = value\n",
    "            loss, penalty = key.split('-')\n",
    "            \n",
    "        \n",
    "                                                                 \n",
    "    \n",
    "    ###\n",
    "    SGD_clf = SGDClassifier(loss = loss, penalty= penalty)\n",
    "    timing_buddy(SGD_clf, 'SGDClassfier', dicto)\n",
    "    confused_buddy(SGD_clf, 'SGDClassfier', dicto)\n",
    "    dicto['SGDClassfier']['ROC_AUC_Score'] = roc_auc_score(y_test, SGD_clf.decision_function(X_test))\n",
    "    dicto['SGDClassfier']['Hyper_Params'] = {'Loss': loss, 'penalty':penalty}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-vampire",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "virgin-tokyo",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_rf_model(dicto, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    def bayes_RFModeler_sqrt(n_estimators, \n",
    "                             max_depth,\n",
    "                             min_samples_split,\n",
    "                             min_samples_leaf):\n",
    "        RandoForest = RandomForestClassifier(n_estimators=int(n_estimators), max_features='sqrt', max_depth=int(max_depth),\n",
    "                                             min_samples_split=int(min_samples_split), min_samples_leaf=int(min_samples_leaf))\n",
    "        RandoForest.fit(X_train, np.ravel(y_train))\n",
    "        confuse = confusion_matrix(y_test, RandoForest.predict(X_test))\n",
    "        recall = recall_calculator(confuse)\n",
    "        return recall\n",
    "\n",
    "    \n",
    "    ###\n",
    "    param_dicts = { 'n_estimators' : (100, 2000),\n",
    "              'max_depth' : (10,60),\n",
    "              'min_samples_split': (2,10),\n",
    "              'min_samples_leaf' : (1,5),\n",
    "              }\n",
    "    ###\n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "    bayes_RFModeler_sqrt,\n",
    "    pbounds=param_dicts,\n",
    "    verbose=1)\n",
    "    \n",
    "    ######\n",
    "    \n",
    "    print('Random Forest')\n",
    "    optimizer.maximize(init_points=15, n_iter=10)\n",
    "    rf_params= optimizer.max\n",
    "    rf_params = rf_params['params']\n",
    "    \n",
    "    ###\n",
    "    RandoForest = RandomForestClassifier(n_estimators=int(rf_params['n_estimators']), \n",
    "                                         max_features='sqrt',\n",
    "                                         max_depth=int(rf_params['max_depth']),\n",
    "                                         min_samples_split=int(rf_params['min_samples_split']),\n",
    "                                         min_samples_leaf=int(rf_params['min_samples_leaf']))\n",
    "    \n",
    "    timing_buddy(RandoForest, 'RandomForestClassifier', dicto)\n",
    "    confused_buddy(RandoForest, 'RandomForestClassifier', dicto)\n",
    "    dicto['RandomForestClassifier']['ROC_AUC_Score'] = roc_auc_score(y_test, RandoForest.predict_proba(X_test)[:, 1])\n",
    "    dicto['RandomForestClassifier']['Hyper_Params'] = rf_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abandoned-performer",
   "metadata": {},
   "source": [
    "## AdaBooast Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "based-hormone",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_hyper_ada_boost_model(dicto, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    def AdaBoost_hyper(n_estimators, learning_rate):\n",
    "        adaira_the_classifier = AdaBoostClassifier(learning_rate=learning_rate,  n_estimators=int(n_estimators))\n",
    "        adaira_the_classifier.fit(X_train, np.ravel(y_train))\n",
    "        confuse = confusion_matrix(y_test, adaira_the_classifier.predict(X_test))\n",
    "        recall = recall_calculator(confuse)\n",
    "        return recall\n",
    "\n",
    "    \n",
    "    ###\n",
    "    ada_params = {'learning_rate' : (.1,2),\n",
    "              'n_estimators':(10,500),}\n",
    "    ###\n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "    AdaBoost_hyper,\n",
    "    pbounds=ada_params,\n",
    "    verbose=1)\n",
    "    \n",
    "    ######\n",
    "    \n",
    "    print('AdaBoost')\n",
    "    optimizer.maximize(init_points=15, n_iter=10)\n",
    "    ada_boost_params= optimizer.max\n",
    "    ada_boost_params = ada_boost_params['params']\n",
    "    \n",
    "    ###\n",
    "    adaira_the_classifier = AdaBoostClassifier(learning_rate=ada_boost_params['learning_rate'],  n_estimators=int(ada_boost_params['n_estimators']))\n",
    "    timing_buddy(adaira_the_classifier, 'AdaBoostClassifier', dicto)\n",
    "    confused_buddy(adaira_the_classifier, 'AdaBoostClassifier', dicto)\n",
    "    dicto['AdaBoostClassifier']['ROC_AUC_Score'] = roc_auc_score(y_test, adaira_the_classifier.predict_proba(X_test)[:, 1])\n",
    "    dicto['AdaBoostClassifier']['Hyper_Params'] = ada_boost_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-short",
   "metadata": {},
   "source": [
    "## Gradiant Boosting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cheap-forwarding",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bayes_hyper_grad_boosting_model(dicto, X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    def GradBoost_hyper(n_estimators, learning_rate, max_leaf_nodes):\n",
    "        grady_the_boosted = GradientBoostingClassifier(learning_rate=learning_rate,  n_estimators=int(n_estimators),\n",
    "                                                      max_leaf_nodes=int(max_leaf_nodes))\n",
    "        grady_the_boosted.fit(X_train, np.ravel(y_train))\n",
    "        confuse = confusion_matrix(y_test, grady_the_boosted.predict(X_test))\n",
    "        recall = recall_calculator(confuse)\n",
    "        return recall\n",
    "    \n",
    "    ###\n",
    "    GradBoost_params = {'learning_rate' : (.1,2),\n",
    "                    'n_estimators':(10,500),\n",
    "                    'max_leaf_nodes':(3,50)}\n",
    "    ###\n",
    "    \n",
    "    optimizer = BayesianOptimization(\n",
    "    GradBoost_hyper,\n",
    "    pbounds=GradBoost_params,\n",
    "    verbose=1)\n",
    "    \n",
    "    ######\n",
    "    \n",
    "    print('Gradient Boosting')\n",
    "    optimizer.maximize(init_points=15, n_iter=10)\n",
    "    grad_boost_params= optimizer.max\n",
    "    grad_boost_params = grad_boost_params['params']\n",
    "    \n",
    "    ###\n",
    "    grady_the_boosted = GradientBoostingClassifier(learning_rate=grad_boost_params['learning_rate'],\n",
    "                                                   n_estimators=int(grad_boost_params['n_estimators']),\n",
    "                                                   max_leaf_nodes=int(grad_boost_params['max_leaf_nodes']))\n",
    "    timing_buddy(grady_the_boosted, 'GradientBoostingClassifier', dicto)\n",
    "    confused_buddy(grady_the_boosted, 'GradientBoostingClassifier', dicto)\n",
    "    dicto['GradientBoostingClassifier']['ROC_AUC_Score'] = roc_auc_score(y_test, grady_the_boosted.predict_proba(X_test)[:, 1])\n",
    "    dicto['GradientBoostingClassifier']['Hyper_Params'] = grad_boost_params\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reported-nashville",
   "metadata": {},
   "source": [
    "## Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "norwegian-procurement",
   "metadata": {},
   "source": [
    "#precision = tp/(tp+fp)\n",
    "maximize this for spam, don't wanna be hiding real emails\n",
    "\n",
    "#recall= tp/(tp+fn) \n",
    "Maximize this for medical or security scenarios, don't wanna miss actual sicknesses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "compact-given",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def medical_evaluator(dicto):\n",
    "    bestmodel = \"\"\n",
    "    training_time= 0\n",
    "    recall = 0\n",
    "    temp_recall = 0\n",
    "    for key, value in dicto.items():\n",
    "        \n",
    "        confuse = dicto[key]['confuse']\n",
    "        if type(confuse) != list:\n",
    "            temp_recall = recall_calculator(confuse)\n",
    "\n",
    "            if temp_recall > recall:\n",
    "\n",
    "                recall = temp_recall\n",
    "                bestmodel = key\n",
    "                training_time = dicto[key]['training_time']\n",
    "    return bestmodel, recall, training_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "expired-danger",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def spam_evaluator(dicto):\n",
    "    bestmodel = \"\"\n",
    "    training_time= 0\n",
    "    precision = 0\n",
    "    temp_precision = 0.00000001\n",
    "    for key, value in dicto.items():\n",
    "        confuse = dicto[key]['confuse']\n",
    "        if type(confuse) != list:\n",
    "            temp_precision = precision_calculator(confuse)\n",
    "\n",
    "            if temp_precision > precision:\n",
    "\n",
    "                precision = temp_precision\n",
    "                bestmodel = key\n",
    "                training_time = dicto[key]['training_time']\n",
    "    return bestmodel, precision, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "provincial-cambridge",
   "metadata": {},
   "source": [
    "# Model Execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-banks",
   "metadata": {},
   "source": [
    "### Aggregate Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "inside-tuning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chonky_model_aggregator(X_train, y_train, X_test, y_test):\n",
    "    \n",
    "    #Models to explore\n",
    "    model_set = ['dummy','', 'SGDClassfier', 'RandomForestClassifier', 'AdaBoostClassifier', 'GradientBoostingClassifier']\n",
    "    model_stats = {}\n",
    "    for each in model_set:\n",
    "        model_stats[each] ={'confuse' : [], 'training_time' : 0, 'ROC_AUC_Score': 0, 'Hyper_Params': {}}\n",
    "    #model running\n",
    "    classy_dummy(model_stats, X_train, y_train, X_test, y_test)\n",
    "    #SVCmodeler(model_stats, X_train, y_train, X_test, y_test)\n",
    "    grid_sgd_model(model_stats, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    #bayes_rf_model(model_stats, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "    bayes_hyper_ada_boost_model(model_stats, X_train, y_train, X_test, y_test)\n",
    "    bayes_hyper_grad_boosting_model(model_stats, X_train, y_train, X_test, y_test)\n",
    "    \n",
    "\n",
    "    return model_stats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "authorized-singing",
   "metadata": {},
   "source": [
    "### Stats Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-smart",
   "metadata": {},
   "source": [
    "#### Wide Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "level-clerk",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dumbo\n",
      "Stochastic Gradiant Descent\n",
      "AdaBoost\n",
      "|   iter    |  target   | learni... | n_esti... |\n",
      "-------------------------------------------------\n",
      "=================================================\n",
      "Gradient Boosting\n",
      "|   iter    |  target   | learni... | max_le... | n_esti... |\n",
      "-------------------------------------------------------------\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.9862  \u001b[0m | \u001b[95m 0.9449  \u001b[0m | \u001b[95m 24.46   \u001b[0m | \u001b[95m 86.76   \u001b[0m |\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.9942  \u001b[0m | \u001b[95m 1.878   \u001b[0m | \u001b[95m 43.54   \u001b[0m | \u001b[95m 48.37   \u001b[0m |\n",
      "| \u001b[95m 7       \u001b[0m | \u001b[95m 0.9943  \u001b[0m | \u001b[95m 0.3165  \u001b[0m | \u001b[95m 29.84   \u001b[0m | \u001b[95m 98.72   \u001b[0m |\n",
      "| \u001b[95m 11      \u001b[0m | \u001b[95m 0.9999  \u001b[0m | \u001b[95m 0.1562  \u001b[0m | \u001b[95m 32.13   \u001b[0m | \u001b[95m 33.3    \u001b[0m |\n",
      "| \u001b[95m 17      \u001b[0m | \u001b[95m 1.0     \u001b[0m | \u001b[95m 0.4163  \u001b[0m | \u001b[95m 3.978   \u001b[0m | \u001b[95m 10.08   \u001b[0m |\n",
      "=============================================================\n"
     ]
    }
   ],
   "source": [
    "wide_data_model_stats = chonky_model_aggregator(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "neutral-chart",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dummy': {'confuse': array([[ 292, 1650],\n",
       "         [1640, 8918]], dtype=int64),\n",
       "  'training_time': 0.003509521484375,\n",
       "  'ROC_AUC_Score': 0.5036965150961518,\n",
       "  'Hyper_Params': {}},\n",
       " 'SVC': {'confuse': [],\n",
       "  'training_time': 0,\n",
       "  'ROC_AUC_Score': 0,\n",
       "  'Hyper_Params': {}},\n",
       " 'SGDClassfier': {'confuse': array([[    0,  1942],\n",
       "         [    0, 10558]], dtype=int64),\n",
       "  'training_time': 0.050966739654541016,\n",
       "  'ROC_AUC_Score': 0.669619744517509,\n",
       "  'Hyper_Params': {'Loss': 'modified_huber', 'penalty': 'l2'}},\n",
       " 'RandomForestClassifier': {'confuse': [],\n",
       "  'training_time': 0,\n",
       "  'ROC_AUC_Score': 0,\n",
       "  'Hyper_Params': {}},\n",
       " 'AdaBoostClassifier': {'confuse': array([[    0,  1942],\n",
       "         [    0, 10558]], dtype=int64),\n",
       "  'training_time': 4.501019239425659,\n",
       "  'ROC_AUC_Score': 0.6877135109109428,\n",
       "  'Hyper_Params': {'learning_rate': 0.2786209262205007,\n",
       "   'n_estimators': 274.02810984445074}},\n",
       " 'GradientBoostingClassifier': {'confuse': array([[    0,  1942],\n",
       "         [    0, 10558]], dtype=int64),\n",
       "  'training_time': 0.22296905517578125,\n",
       "  'ROC_AUC_Score': 0.6833459928765806,\n",
       "  'Hyper_Params': {'learning_rate': 0.4163419878067033,\n",
       "   'max_leaf_nodes': 3.977698440727402,\n",
       "   'n_estimators': 10.075609959280012}}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wide_data_model_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "absolute-vertex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AdaBoostClassifier': {'Hyper_Params': {'learning_rate': 0.2786209262205007,\n",
      "                                         'n_estimators': 274.02810984445074},\n",
      "                        'ROC_AUC_Score': 0.6877135109109428,\n",
      "                        'confuse': array([[    0,  1942],\n",
      "       [    0, 10558]], dtype=int64),\n",
      "                        'training_time': 4.501019239425659},\n",
      " 'GradientBoostingClassifier': {'Hyper_Params': {'learning_rate': 0.4163419878067033,\n",
      "                                                 'max_leaf_nodes': 3.977698440727402,\n",
      "                                                 'n_estimators': 10.075609959280012},\n",
      "                                'ROC_AUC_Score': 0.6833459928765806,\n",
      "                                'confuse': array([[    0,  1942],\n",
      "       [    0, 10558]], dtype=int64),\n",
      "                                'training_time': 0.22296905517578125},\n",
      " 'SGDClassfier': {'Hyper_Params': {'Loss': 'modified_huber', 'penalty': 'l2'},\n",
      "                  'ROC_AUC_Score': 0.669619744517509,\n",
      "                  'confuse': array([[    0,  1942],\n",
      "       [    0, 10558]], dtype=int64),\n",
      "                  'training_time': 0.050966739654541016},\n",
      " 'SVC': {'Hyper_Params': {},\n",
      "         'ROC_AUC_Score': 0,\n",
      "         'confuse': [],\n",
      "         'training_time': 0},\n",
      " 'dummy': {'Hyper_Params': {},\n",
      "           'ROC_AUC_Score': 0.5036965150961518,\n",
      "           'confuse': array([[ 292, 1650],\n",
      "       [1640, 8918]], dtype=int64),\n",
      "           'training_time': 0.003509521484375}}\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(wide_data_model_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "virgin-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "del wide_data_model_stats['RandomForestClassifier']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "phantom-chorus",
   "metadata": {},
   "outputs": [],
   "source": [
    "del wide_data_model_stats['SVC']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "martial-picture",
   "metadata": {},
   "source": [
    "# Stats Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "reasonable-enough",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('SGDClassfier', 1.0, 0.050966739654541016)\n"
     ]
    }
   ],
   "source": [
    "print(medical_evaluator(wide_data_model_stats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "figured-dining",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dummy                         0.003510\n",
       "SGDClassfier                  0.050967\n",
       "GradientBoostingClassifier    0.222969\n",
       "AdaBoostClassifier            4.501019\n",
       "dtype: float64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_dict= {}\n",
    "for key, value in wide_data_model_stats.items():\n",
    "    time_dict[key] = value['training_time']\n",
    "    time_list = value['training_time']\n",
    "\n",
    "time_frame = pd.Series(time_dict)\n",
    "time_frame.sort_values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "given-weekend",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 292 1650]\n",
      " [1640 8918]]\n",
      "[[    0  1942]\n",
      " [    0 10558]]\n",
      "[[    0  1942]\n",
      " [    0 10558]]\n",
      "[[    0  1942]\n",
      " [    0 10558]]\n"
     ]
    }
   ],
   "source": [
    "confuse_dict= {}\n",
    "for key, value in wide_data_model_stats.items():\n",
    "    confuse_dict[key] = value['confuse']\n",
    "\n",
    "confuse_frame = pd.Series(confuse_dict)\n",
    "\n",
    "precision_dict= {}\n",
    "recall_dict = {}\n",
    "for i in confuse_frame.index:\n",
    "    precision_dict[i] = precision_calculator(confuse_frame[i])        \n",
    "    recall_dict[i] = recall_calculator(confuse_frame[i])        \n",
    "\n",
    "precision_frame = pd.Series(precision_dict)\n",
    "\n",
    "\n",
    "recall_frame = pd.Series(recall_dict)\n",
    "\n",
    "recall_frame = dict(recall_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "consecutive-thunder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Recall</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Time_To_Train</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>dummy</th>\n",
       "      <td>0.844668</td>\n",
       "      <td>0.843868</td>\n",
       "      <td>0.003510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SGDClassfier</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.844640</td>\n",
       "      <td>0.050967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GradientBoostingClassifier</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.844640</td>\n",
       "      <td>0.222969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AdaBoostClassifier</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.844640</td>\n",
       "      <td>4.501019</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Recall  Precision  Time_To_Train\n",
       "dummy                       0.844668   0.843868       0.003510\n",
       "SGDClassfier                1.000000   0.844640       0.050967\n",
       "GradientBoostingClassifier  1.000000   0.844640       0.222969\n",
       "AdaBoostClassifier          1.000000   0.844640       4.501019"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "working_list = (recall_frame, precision_frame, time_frame)\n",
    "list_of_three = pd.DataFrame(data=working_list, index=['Recall','Precision','Time_To_Train'])\n",
    "list_of_three= list_of_three.transpose()\n",
    "list_of_three.loc['GradientBoostingClassifier'] #recall\n",
    "#list_of_three.loc['SVC'] #precision\n",
    "list_of_three.loc['SGDClassfier'] #time\n",
    "#list_of_three.sort_values('Precision', ascending=False)\n",
    "list_of_three.sort_values('Time_To_Train')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "respective-dancing",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 292 1650]\n",
      " [1640 8918]]\n",
      "[[    0  1942]\n",
      " [    0 10558]]\n",
      "[[    0  1942]\n",
      " [    0 10558]]\n",
      "[[    0  1942]\n",
      " [    0 10558]]\n",
      "The best medical model (Optimizing for recall) is SGDClassfier It's recall score was1.0 and it took 0.05 seconds to run.\n",
      "\n",
      "The best spam model (Optimizing for precision) is SGDClassfier It's precision score was0.845 and it took 0.05 seconds to run.\n"
     ]
    }
   ],
   "source": [
    "best_med_model, med_recall, med_time = medical_evaluator(wide_data_model_stats)\n",
    "best_spam_model, spam_precision, spam_time = spam_evaluator(wide_data_model_stats)\n",
    "print(\"The best medical model (Optimizing for recall) is \" + best_med_model + \" It's recall score was\" + str(round(med_recall,3)) + \" and it took \" + str(round(med_time,2)) +\" seconds to run.\")\n",
    "print(\"\")\n",
    "print(\"The best spam model (Optimizing for precision) is \" + best_spam_model +\n",
    "      \" It's precision score was\" +str(round(spam_precision, 3)) +\" and it took \" + str(round(spam_time, 2) ) +\" seconds to run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "artificial-channels",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dummy\n",
      "\n",
      "confuse\n",
      "recall: 0.84\n",
      "[[ 292 1650]\n",
      " [1640 8918]]\n",
      "precision: 0.84\n",
      "[[ 292 1650]\n",
      " [1640 8918]]\n",
      "\n",
      "training_time\n",
      "0.003509521484375\n",
      "\n",
      "ROC_AUC_Score\n",
      "0.5036965150961518\n",
      "\n",
      "Hyper_Params\n",
      "{}\n",
      "\n",
      "\n",
      "\n",
      "SGDClassfier\n",
      "\n",
      "confuse\n",
      "recall: 1.0\n",
      "[[    0  1942]\n",
      " [    0 10558]]\n",
      "precision: 0.84\n",
      "[[    0  1942]\n",
      " [    0 10558]]\n",
      "\n",
      "training_time\n",
      "0.050966739654541016\n",
      "\n",
      "ROC_AUC_Score\n",
      "0.669619744517509\n",
      "\n",
      "Hyper_Params\n",
      "{'Loss': 'modified_huber', 'penalty': 'l2'}\n",
      "\n",
      "\n",
      "\n",
      "AdaBoostClassifier\n",
      "\n",
      "confuse\n",
      "recall: 1.0\n",
      "[[    0  1942]\n",
      " [    0 10558]]\n",
      "precision: 0.84\n",
      "[[    0  1942]\n",
      " [    0 10558]]\n",
      "\n",
      "training_time\n",
      "4.501019239425659\n",
      "\n",
      "ROC_AUC_Score\n",
      "0.6877135109109428\n",
      "\n",
      "Hyper_Params\n",
      "{'learning_rate': 0.2786209262205007, 'n_estimators': 274.02810984445074}\n",
      "\n",
      "\n",
      "\n",
      "GradientBoostingClassifier\n",
      "\n",
      "confuse\n",
      "recall: 1.0\n",
      "[[    0  1942]\n",
      " [    0 10558]]\n",
      "precision: 0.84\n",
      "[[    0  1942]\n",
      " [    0 10558]]\n",
      "\n",
      "training_time\n",
      "0.22296905517578125\n",
      "\n",
      "ROC_AUC_Score\n",
      "0.6833459928765806\n",
      "\n",
      "Hyper_Params\n",
      "{'learning_rate': 0.4163419878067033, 'max_leaf_nodes': 3.977698440727402, 'n_estimators': 10.075609959280012}\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key, value in wide_data_model_stats.items():\n",
    "    if value['training_time'] !=0:\n",
    "        print(key)\n",
    "        print('')\n",
    "        for key2, value2 in value.items():\n",
    "            print(key2)\n",
    "            if key2 == 'confuse':\n",
    "                if type(value2) != list:\n",
    "                    print('recall: ' + str(round(recall_calculator(value2),2)))\n",
    "                    print('precision: ' +str(round(precision_calculator(value2),2)))\n",
    "            print(value2)\n",
    "            print('')\n",
    "        print('')\n",
    "        print('')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "registered-spiritual",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save this iteration.\n",
    "\n",
    "pickle.dump( wide_data_model_stats, open( \"model_stats_02122022.p\", \"wb\" ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-greensboro",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickles = ['model_stats_01012022.p','model_stats_01022022.p', 'model_stats_01032022.p', 'model_stats_01222022.p', \"model_stats_01242022.p\", \"model_stats_01252022.p\", \"model_stats_01262022.p\", \"model_stats_02122022.p\"]\n",
    "unpickles = []\n",
    "for each in pickles:\n",
    "    file = open(each, 'rb')\n",
    "    unpickles.append(pickle.load(file))\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "executed-ownership",
   "metadata": {},
   "outputs": [],
   "source": [
    "for each in unpickles:\n",
    "    best_spam_model, spam_precision, spam_time = spam_evaluator(each)\n",
    "    print(best_spam_model, round(spam_precision,2), round(spam_time,2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
